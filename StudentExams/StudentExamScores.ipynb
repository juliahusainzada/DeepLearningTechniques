{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, root_mean_squared_error, r2_score, mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understand the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 200 entries, 0 to 199\n",
      "Data columns (total 6 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   student_id          200 non-null    object \n",
      " 1   hours_studied       200 non-null    float64\n",
      " 2   sleep_hours         200 non-null    float64\n",
      " 3   attendance_percent  200 non-null    float64\n",
      " 4   previous_scores     200 non-null    int64  \n",
      " 5   exam_score          200 non-null    float64\n",
      "dtypes: float64(4), int64(1), object(1)\n",
      "memory usage: 9.5+ KB\n",
      "None\n",
      "----------------------------------------------\n",
      "       hours_studied  sleep_hours  attendance_percent  previous_scores  \\\n",
      "count     200.000000   200.000000          200.000000       200.000000   \n",
      "mean        6.325500     6.622000           74.830000        66.800000   \n",
      "std         3.227317     1.497138           14.249905        15.663869   \n",
      "min         1.000000     4.000000           50.300000        40.000000   \n",
      "25%         3.500000     5.300000           62.200000        54.000000   \n",
      "50%         6.150000     6.700000           75.250000        67.500000   \n",
      "75%         9.000000     8.025000           87.425000        80.000000   \n",
      "max        12.000000     9.000000          100.000000        95.000000   \n",
      "\n",
      "       exam_score  \n",
      "count  200.000000  \n",
      "mean    33.955000  \n",
      "std      6.789548  \n",
      "min     17.100000  \n",
      "25%     29.500000  \n",
      "50%     34.050000  \n",
      "75%     38.750000  \n",
      "max     51.300000  \n",
      "----------------------------------------------\n",
      "student_id            0\n",
      "hours_studied         0\n",
      "sleep_hours           0\n",
      "attendance_percent    0\n",
      "previous_scores       0\n",
      "exam_score            0\n",
      "dtype: int64\n",
      "----------------------------------------------\n",
      "Pearson correlation with exam_score:\n",
      " exam_score            1.000000\n",
      "hours_studied         0.776751\n",
      "previous_scores       0.431105\n",
      "attendance_percent    0.225713\n",
      "sleep_hours           0.188222\n",
      "Name: exam_score, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('student_exam_scores.csv')\n",
    "\n",
    "print(df.info())\n",
    "print(\"----------------------------------------------\")\n",
    "\n",
    "print(df.describe())\n",
    "print(\"----------------------------------------------\")\n",
    "\n",
    "print(df.isnull().sum())\n",
    "print(\"----------------------------------------------\")\n",
    "\n",
    "pearson = df.corr(numeric_only=True)['exam_score'].sort_values(ascending=False)\n",
    "print(\"Pearson correlation with exam_score:\\n\", pearson)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hours_studied', 'sleep_hours', 'attendance_percent', 'previous_scores']\n"
     ]
    }
   ],
   "source": [
    "# Target \n",
    "label = \"exam_score\"\n",
    "id_col = [\"student_id\"]\n",
    "\n",
    "# Select features (for now)\n",
    "cols = [c for c in df.columns if c not in id_col + [label]] \n",
    "print(cols)\n",
    "\n",
    "X = df.drop(columns = id_col + [label])\n",
    "y = df[label].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining a baseline model for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression | MAE=2.311  RMSE=2.786  R²=0.854\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Linear Regression baseline\n",
    "lin = LinearRegression()\n",
    "lin.fit(X_train, y_train)\n",
    "pred_lin = lin.predict(X_test)\n",
    "\n",
    "print(f\"Linear Regression | \"\n",
    "      f\"MAE={mean_absolute_error(y_test, pred_lin):.3f}  \"\n",
    "      f\"RMSE={root_mean_squared_error(y_test, pred_lin):.3f}  \"\n",
    "      f\"R²={r2_score(y_test, pred_lin):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- MAE = 2.311: On average this model’s predictions are about 2.3 points away from the true exam score\n",
    "- RMSE = 2.786: The “typical” error magnitude (giving more weight to big misses) is about 2.8 points\n",
    "- R² = 0.854: 85.4% of the variation in exam scores can be explained by the features (hours studied, sleep, attendance, previous scores, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's design our neural network model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 160 entries, 79 to 102\n",
      "Data columns (total 4 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   hours_studied       160 non-null    float64\n",
      " 1   sleep_hours         160 non-null    float64\n",
      " 2   attendance_percent  160 non-null    float64\n",
      " 3   previous_scores     160 non-null    int64  \n",
      "dtypes: float64(3), int64(1)\n",
      "memory usage: 6.2 KB\n",
      "None\n",
      "(160, 4)\n",
      "(40, 4)\n"
     ]
    }
   ],
   "source": [
    "X_train = X_train.copy()\n",
    "X_test = X_test.copy()\n",
    "\n",
    "print(X_train.info())\n",
    "\n",
    "Xtr = X_train.to_numpy(dtype=np.float32)\n",
    "Xte = X_test.to_numpy(dtype=np.float32)\n",
    "ytr = y_train.to_numpy(dtype=np.float32)\n",
    "yte = y_test.to_numpy(dtype=np.float32)\n",
    "\n",
    "print(Xtr.shape)\n",
    "print(Xte.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.set_random_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **Initial 4-Layer Network**\n",
    "   - Input layer → 64 neurons → 32 neurons → 16 neurons → Output (linear)\n",
    "   - Activation: ReLU for hidden layers\n",
    "   - Test with learning rates 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - loss: 74.5906 - mae: 74.5906 - rmse: 118.2237 - val_loss: 27.0699 - val_mae: 27.0699 - val_rmse: 27.8194\n",
      "Epoch 2/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 15.1706 - mae: 15.1706 - rmse: 17.8433 - val_loss: 11.5301 - val_mae: 11.5301 - val_rmse: 12.7015\n",
      "Epoch 3/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 8.7511 - mae: 8.7511 - rmse: 10.2844 - val_loss: 5.7981 - val_mae: 5.7981 - val_rmse: 7.1905\n",
      "Epoch 4/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 8.3175 - mae: 8.3175 - rmse: 9.7352 - val_loss: 3.9610 - val_mae: 3.9610 - val_rmse: 4.8802\n",
      "Epoch 5/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 6.1758 - mae: 6.1758 - rmse: 7.6404 - val_loss: 8.5857 - val_mae: 8.5857 - val_rmse: 9.7647\n",
      "Epoch 6/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 6.3635 - mae: 6.3635 - rmse: 7.6003 - val_loss: 7.8694 - val_mae: 7.8694 - val_rmse: 8.9118\n",
      "Epoch 7/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 6.5238 - mae: 6.5238 - rmse: 7.6850 - val_loss: 6.0962 - val_mae: 6.0962 - val_rmse: 7.2017\n",
      "Epoch 8/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 4.6731 - mae: 4.6731 - rmse: 5.7357 - val_loss: 4.0581 - val_mae: 4.0581 - val_rmse: 5.0699\n",
      "Epoch 9/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 3.6721 - mae: 3.6721 - rmse: 4.5766 - val_loss: 3.7741 - val_mae: 3.7741 - val_rmse: 4.5826\n",
      "Epoch 10/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 3.9475 - mae: 3.9475 - rmse: 4.8582 - val_loss: 2.9959 - val_mae: 2.9959 - val_rmse: 3.8263\n",
      "Epoch 11/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 3.2977 - mae: 3.2977 - rmse: 4.0864 - val_loss: 3.4925 - val_mae: 3.4925 - val_rmse: 4.4141\n",
      "Epoch 12/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 3.0046 - mae: 3.0046 - rmse: 3.8343 - val_loss: 2.6499 - val_mae: 2.6499 - val_rmse: 3.3772\n",
      "Epoch 13/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.6813 - mae: 2.6813 - rmse: 3.3806 - val_loss: 4.1091 - val_mae: 4.1091 - val_rmse: 4.8510\n",
      "Epoch 14/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 3.7186 - mae: 3.7186 - rmse: 4.3557 - val_loss: 2.7635 - val_mae: 2.7635 - val_rmse: 3.2493\n",
      "Epoch 15/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.8550 - mae: 2.8550 - rmse: 3.4645 - val_loss: 2.3472 - val_mae: 2.3472 - val_rmse: 2.7561\n",
      "Epoch 16/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.4289 - mae: 2.4289 - rmse: 2.9165 - val_loss: 2.6450 - val_mae: 2.6450 - val_rmse: 3.1168\n",
      "Epoch 17/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.6727 - mae: 2.6727 - rmse: 3.1474 - val_loss: 3.4366 - val_mae: 3.4366 - val_rmse: 4.0869\n",
      "Epoch 18/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 3.3294 - mae: 3.3294 - rmse: 3.9053 - val_loss: 2.2837 - val_mae: 2.2837 - val_rmse: 2.7031\n",
      "Epoch 19/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.7710 - mae: 2.7710 - rmse: 3.4212 - val_loss: 2.1466 - val_mae: 2.1466 - val_rmse: 2.6661\n",
      "Epoch 20/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.4636 - mae: 2.4636 - rmse: 3.0644 - val_loss: 2.1500 - val_mae: 2.1500 - val_rmse: 2.6592\n",
      "Epoch 21/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.3963 - mae: 2.3963 - rmse: 2.8509 - val_loss: 2.9130 - val_mae: 2.9130 - val_rmse: 3.4183\n",
      "Epoch 22/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.8591 - mae: 2.8591 - rmse: 3.3419 - val_loss: 3.0867 - val_mae: 3.0867 - val_rmse: 3.6585\n",
      "Epoch 23/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.9122 - mae: 2.9122 - rmse: 3.4955 - val_loss: 2.2464 - val_mae: 2.2464 - val_rmse: 2.7375\n",
      "Epoch 24/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.3959 - mae: 2.3959 - rmse: 2.8833 - val_loss: 2.5262 - val_mae: 2.5262 - val_rmse: 2.9567\n",
      "Epoch 25/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.6558 - mae: 2.6558 - rmse: 3.1460 - val_loss: 4.1754 - val_mae: 4.1754 - val_rmse: 4.8236\n",
      "Epoch 26/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 3.5541 - mae: 3.5541 - rmse: 4.1638 - val_loss: 2.1461 - val_mae: 2.1461 - val_rmse: 2.6417\n",
      "Epoch 27/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.8517 - mae: 2.8517 - rmse: 3.5540 - val_loss: 2.5151 - val_mae: 2.5151 - val_rmse: 3.1921\n",
      "Epoch 28/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 3.0707 - mae: 3.0707 - rmse: 3.8769 - val_loss: 3.1517 - val_mae: 3.1517 - val_rmse: 3.8713\n",
      "Epoch 29/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.7347 - mae: 2.7347 - rmse: 3.4183 - val_loss: 2.6705 - val_mae: 2.6705 - val_rmse: 3.3573\n",
      "Epoch 30/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.4440 - mae: 2.4440 - rmse: 2.9830 - val_loss: 2.1537 - val_mae: 2.1537 - val_rmse: 2.6899\n",
      "Epoch 31/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.3504 - mae: 2.3504 - rmse: 2.8438 - val_loss: 2.5938 - val_mae: 2.5938 - val_rmse: 3.0172\n",
      "Epoch 32/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.5905 - mae: 2.5905 - rmse: 3.1280 - val_loss: 3.8112 - val_mae: 3.8112 - val_rmse: 4.4752\n",
      "Epoch 33/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 3.4316 - mae: 3.4316 - rmse: 4.0104 - val_loss: 2.6319 - val_mae: 2.6319 - val_rmse: 3.0524\n",
      "Epoch 34/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.7323 - mae: 2.7323 - rmse: 3.3541 - val_loss: 2.1535 - val_mae: 2.1535 - val_rmse: 2.6888\n",
      "Epoch 35/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 2.4029 - mae: 2.4029 - rmse: 2.9285 - val_loss: 2.1881 - val_mae: 2.1881 - val_rmse: 2.6880\n",
      "Epoch 36/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.4270 - mae: 2.4270 - rmse: 2.8607 - val_loss: 3.2169 - val_mae: 3.2169 - val_rmse: 3.7958\n",
      "Epoch 37/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 3.0673 - mae: 3.0673 - rmse: 3.6214 - val_loss: 3.2224 - val_mae: 3.2224 - val_rmse: 3.8118\n",
      "Epoch 38/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 3.0379 - mae: 3.0379 - rmse: 3.5556 - val_loss: 2.2302 - val_mae: 2.2302 - val_rmse: 2.7123\n",
      "Epoch 39/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.4929 - mae: 2.4929 - rmse: 3.0726 - val_loss: 2.2165 - val_mae: 2.2165 - val_rmse: 2.7082\n",
      "Epoch 40/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.3255 - mae: 2.3255 - rmse: 2.7852 - val_loss: 2.6891 - val_mae: 2.6891 - val_rmse: 3.1016\n",
      "Epoch 41/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.6689 - mae: 2.6689 - rmse: 3.1689 - val_loss: 3.7227 - val_mae: 3.7227 - val_rmse: 4.4234\n",
      "Epoch 42/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 3.2551 - mae: 3.2551 - rmse: 3.8203 - val_loss: 2.4654 - val_mae: 2.4654 - val_rmse: 2.8819\n",
      "Epoch 43/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.5139 - mae: 2.5139 - rmse: 3.1070 - val_loss: 2.2169 - val_mae: 2.2169 - val_rmse: 2.6883\n",
      "Epoch 44/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.3244 - mae: 2.3244 - rmse: 2.7879 - val_loss: 2.7439 - val_mae: 2.7439 - val_rmse: 3.1593\n",
      "Epoch 45/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.6661 - mae: 2.6661 - rmse: 3.1685 - val_loss: 3.5940 - val_mae: 3.5940 - val_rmse: 4.2784\n",
      "Epoch 46/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 3.2432 - mae: 3.2432 - rmse: 3.8046 - val_loss: 2.5484 - val_mae: 2.5484 - val_rmse: 2.9594\n",
      "Epoch 47/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.5638 - mae: 2.5638 - rmse: 3.1463 - val_loss: 2.3129 - val_mae: 2.3129 - val_rmse: 2.7573\n",
      "Epoch 48/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.4413 - mae: 2.4413 - rmse: 2.8931 - val_loss: 2.8836 - val_mae: 2.8836 - val_rmse: 3.3380\n",
      "Epoch 49/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.7231 - mae: 2.7231 - rmse: 3.2395 - val_loss: 3.3875 - val_mae: 3.3875 - val_rmse: 4.0171\n",
      "Epoch 50/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 3.0791 - mae: 3.0791 - rmse: 3.6184 - val_loss: 2.5240 - val_mae: 2.5240 - val_rmse: 2.9315\n",
      "Epoch 51/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.4644 - mae: 2.4644 - rmse: 3.0232 - val_loss: 2.4332 - val_mae: 2.4332 - val_rmse: 2.8566\n",
      "Epoch 52/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.5278 - mae: 2.5278 - rmse: 3.0022 - val_loss: 2.9195 - val_mae: 2.9195 - val_rmse: 3.3864\n",
      "Epoch 53/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.9239 - mae: 2.9239 - rmse: 3.4584 - val_loss: 2.7194 - val_mae: 2.7194 - val_rmse: 3.1304\n",
      "Epoch 54/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.5789 - mae: 2.5789 - rmse: 3.1695 - val_loss: 2.3119 - val_mae: 2.3119 - val_rmse: 2.7697\n",
      "Epoch 55/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.5576 - mae: 2.5576 - rmse: 3.0210 - val_loss: 2.7009 - val_mae: 2.7009 - val_rmse: 3.1108\n",
      "Epoch 56/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 2.5457 - mae: 2.5457 - rmse: 3.0520 - val_loss: 2.5497 - val_mae: 2.5497 - val_rmse: 2.9572\n",
      "Epoch 57/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.5543 - mae: 2.5543 - rmse: 3.0416 - val_loss: 2.6513 - val_mae: 2.6513 - val_rmse: 3.1091\n",
      "Epoch 58/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.6781 - mae: 2.6781 - rmse: 3.1669 - val_loss: 2.9552 - val_mae: 2.9552 - val_rmse: 3.4396\n",
      "Epoch 59/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.7891 - mae: 2.7891 - rmse: 3.2950 - val_loss: 2.4872 - val_mae: 2.4872 - val_rmse: 2.9230\n",
      "Epoch 60/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.5896 - mae: 2.5896 - rmse: 3.0463 - val_loss: 2.8719 - val_mae: 2.8719 - val_rmse: 3.3322\n",
      "Epoch 61/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.7868 - mae: 2.7868 - rmse: 3.2727 - val_loss: 2.5789 - val_mae: 2.5789 - val_rmse: 2.9986\n",
      "Epoch 62/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.5573 - mae: 2.5573 - rmse: 3.0407 - val_loss: 2.8502 - val_mae: 2.8502 - val_rmse: 3.2922\n",
      "Epoch 63/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.7136 - mae: 2.7136 - rmse: 3.2642 - val_loss: 3.2330 - val_mae: 3.2330 - val_rmse: 3.8535\n",
      "Epoch 64/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 3.1860 - mae: 3.1860 - rmse: 3.7473 - val_loss: 2.4496 - val_mae: 2.4496 - val_rmse: 2.8609\n",
      "Epoch 65/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.5466 - mae: 2.5466 - rmse: 3.1640 - val_loss: 2.3465 - val_mae: 2.3465 - val_rmse: 2.8104\n",
      "Epoch 66/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.4107 - mae: 2.4107 - rmse: 2.8918 - val_loss: 2.8296 - val_mae: 2.8296 - val_rmse: 3.2805\n",
      "Epoch 67/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.7354 - mae: 2.7354 - rmse: 3.2736 - val_loss: 3.3078 - val_mae: 3.3078 - val_rmse: 3.9260\n",
      "Epoch 68/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 3.0689 - mae: 3.0689 - rmse: 3.6160 - val_loss: 2.2012 - val_mae: 2.2012 - val_rmse: 2.7315\n",
      "Epoch 69/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.4441 - mae: 2.4441 - rmse: 3.0544 - val_loss: 2.2192 - val_mae: 2.2192 - val_rmse: 2.7492\n",
      "Epoch 70/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 2.3276 - mae: 2.3276 - rmse: 2.8106 - val_loss: 2.6980 - val_mae: 2.6980 - val_rmse: 3.1247\n",
      "Epoch 71/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.6767 - mae: 2.6767 - rmse: 3.2493 - val_loss: 4.1311 - val_mae: 4.1311 - val_rmse: 4.8188\n",
      "Epoch 72/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 3.6405 - mae: 3.6405 - rmse: 4.2744 - val_loss: 2.2060 - val_mae: 2.2060 - val_rmse: 2.7601\n",
      "Epoch 73/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.8390 - mae: 2.8390 - rmse: 3.6255 - val_loss: 2.5797 - val_mae: 2.5797 - val_rmse: 3.2262\n",
      "Epoch 74/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.7052 - mae: 2.7052 - rmse: 3.4654 - val_loss: 2.9379 - val_mae: 2.9379 - val_rmse: 3.6645\n",
      "Epoch 75/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.5653 - mae: 2.5653 - rmse: 3.1267 - val_loss: 2.2445 - val_mae: 2.2445 - val_rmse: 2.8556\n",
      "Epoch 76/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.4332 - mae: 2.4332 - rmse: 3.0222 - val_loss: 3.0470 - val_mae: 3.0470 - val_rmse: 3.5742\n",
      "Epoch 77/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 3.0307 - mae: 3.0307 - rmse: 3.5673 - val_loss: 2.8330 - val_mae: 2.8330 - val_rmse: 3.2915\n",
      "Epoch 78/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.6584 - mae: 2.6584 - rmse: 3.2016 - val_loss: 2.6074 - val_mae: 2.6074 - val_rmse: 3.0351\n",
      "Epoch 79/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.6493 - mae: 2.6493 - rmse: 3.1153 - val_loss: 2.7791 - val_mae: 2.7791 - val_rmse: 3.2120\n",
      "Epoch 80/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.5711 - mae: 2.5711 - rmse: 3.1312 - val_loss: 2.4374 - val_mae: 2.4374 - val_rmse: 2.8359\n",
      "Epoch 81/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.5197 - mae: 2.5197 - rmse: 3.0225 - val_loss: 2.6048 - val_mae: 2.6048 - val_rmse: 3.0576\n",
      "Epoch 82/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.6455 - mae: 2.6455 - rmse: 3.1288 - val_loss: 2.7957 - val_mae: 2.7957 - val_rmse: 3.2283\n",
      "Epoch 83/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.5734 - mae: 2.5734 - rmse: 3.1514 - val_loss: 2.5291 - val_mae: 2.5291 - val_rmse: 2.9586\n",
      "Epoch 84/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.5612 - mae: 2.5612 - rmse: 3.0307 - val_loss: 2.9695 - val_mae: 2.9695 - val_rmse: 3.4480\n",
      "Epoch 85/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.8385 - mae: 2.8385 - rmse: 3.3579 - val_loss: 2.3632 - val_mae: 2.3632 - val_rmse: 2.8071\n",
      "Epoch 86/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.3679 - mae: 2.3679 - rmse: 2.8873 - val_loss: 2.5267 - val_mae: 2.5267 - val_rmse: 2.9394\n",
      "Epoch 87/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.5284 - mae: 2.5284 - rmse: 3.0517 - val_loss: 2.5509 - val_mae: 2.5509 - val_rmse: 2.9917\n",
      "Epoch 88/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.5403 - mae: 2.5403 - rmse: 3.0635 - val_loss: 3.5467 - val_mae: 3.5467 - val_rmse: 4.1906\n",
      "Epoch 89/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 3.2774 - mae: 3.2774 - rmse: 3.8684 - val_loss: 2.3074 - val_mae: 2.3074 - val_rmse: 2.7909\n",
      "Epoch 90/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.5375 - mae: 2.5375 - rmse: 3.1906 - val_loss: 2.1565 - val_mae: 2.1565 - val_rmse: 2.7418\n",
      "Epoch 91/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.3077 - mae: 2.3077 - rmse: 2.8780 - val_loss: 2.2124 - val_mae: 2.2124 - val_rmse: 2.7514\n",
      "Epoch 92/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.4269 - mae: 2.4269 - rmse: 2.9622 - val_loss: 3.5471 - val_mae: 3.5471 - val_rmse: 4.1823\n",
      "Epoch 93/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 3.2783 - mae: 3.2783 - rmse: 3.8667 - val_loss: 2.5460 - val_mae: 2.5460 - val_rmse: 2.9741\n",
      "Epoch 94/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.5652 - mae: 2.5652 - rmse: 3.1987 - val_loss: 2.5227 - val_mae: 2.5227 - val_rmse: 2.9513\n",
      "Epoch 95/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.5106 - mae: 2.5106 - rmse: 2.9967 - val_loss: 3.0009 - val_mae: 3.0009 - val_rmse: 3.4944\n",
      "Epoch 96/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.8563 - mae: 2.8563 - rmse: 3.3903 - val_loss: 2.4697 - val_mae: 2.4697 - val_rmse: 2.9000\n",
      "Epoch 97/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.4431 - mae: 2.4431 - rmse: 2.9787 - val_loss: 2.7532 - val_mae: 2.7532 - val_rmse: 3.1796\n",
      "Epoch 98/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.7080 - mae: 2.7080 - rmse: 3.2181 - val_loss: 2.6587 - val_mae: 2.6587 - val_rmse: 3.0771\n",
      "Epoch 99/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.5179 - mae: 2.5179 - rmse: 3.0589 - val_loss: 2.9056 - val_mae: 2.9056 - val_rmse: 3.3656\n",
      "Epoch 100/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.8448 - mae: 2.8448 - rmse: 3.3861 - val_loss: 2.5521 - val_mae: 2.5521 - val_rmse: 2.9545\n",
      "Epoch 101/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.5434 - mae: 2.5434 - rmse: 3.1554 - val_loss: 2.3942 - val_mae: 2.3942 - val_rmse: 2.8188\n",
      "Epoch 102/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.4097 - mae: 2.4097 - rmse: 2.9057 - val_loss: 2.6510 - val_mae: 2.6510 - val_rmse: 3.0704\n",
      "Epoch 103/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.6155 - mae: 2.6155 - rmse: 3.1221 - val_loss: 2.7731 - val_mae: 2.7731 - val_rmse: 3.2005\n",
      "Epoch 104/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.5707 - mae: 2.5707 - rmse: 3.1189 - val_loss: 2.7760 - val_mae: 2.7760 - val_rmse: 3.2042\n",
      "Epoch 105/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.7115 - mae: 2.7115 - rmse: 3.2361 - val_loss: 2.7457 - val_mae: 2.7457 - val_rmse: 3.1697\n",
      "Epoch 106/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.5890 - mae: 2.5890 - rmse: 3.1546 - val_loss: 2.6459 - val_mae: 2.6459 - val_rmse: 3.0593\n",
      "Epoch 107/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.6677 - mae: 2.6677 - rmse: 3.1824 - val_loss: 2.4983 - val_mae: 2.4983 - val_rmse: 2.9100\n",
      "Epoch 108/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.4257 - mae: 2.4257 - rmse: 2.9894 - val_loss: 2.3938 - val_mae: 2.3938 - val_rmse: 2.8176\n",
      "Epoch 109/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.4367 - mae: 2.4367 - rmse: 2.9771 - val_loss: 3.0964 - val_mae: 3.0964 - val_rmse: 3.6095\n",
      "Epoch 110/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.9379 - mae: 2.9379 - rmse: 3.5066 - val_loss: 3.0155 - val_mae: 3.0155 - val_rmse: 3.5119\n",
      "Epoch 111/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.8378 - mae: 2.8378 - rmse: 3.4027 - val_loss: 2.3821 - val_mae: 2.3821 - val_rmse: 2.8125\n",
      "Epoch 112/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 2.3965 - mae: 2.3965 - rmse: 2.9285 - val_loss: 2.9681 - val_mae: 2.9681 - val_rmse: 3.4396\n",
      "Epoch 113/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.7123 - mae: 2.7123 - rmse: 3.2466 - val_loss: 3.1113 - val_mae: 3.1113 - val_rmse: 3.6296\n",
      "Epoch 114/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.8671 - mae: 2.8671 - rmse: 3.4202 - val_loss: 2.4997 - val_mae: 2.4997 - val_rmse: 2.9205\n",
      "Epoch 115/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 2.4459 - mae: 2.4459 - rmse: 2.9948 - val_loss: 2.7301 - val_mae: 2.7301 - val_rmse: 3.1493\n",
      "Epoch 116/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.5650 - mae: 2.5650 - rmse: 3.0809 - val_loss: 2.9397 - val_mae: 2.9397 - val_rmse: 3.4094\n",
      "Epoch 117/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.8665 - mae: 2.8665 - rmse: 3.4210 - val_loss: 2.3618 - val_mae: 2.3618 - val_rmse: 2.7880\n",
      "Epoch 118/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.3644 - mae: 2.3644 - rmse: 2.9266 - val_loss: 2.5141 - val_mae: 2.5141 - val_rmse: 2.9339\n",
      "Epoch 119/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.6050 - mae: 2.6050 - rmse: 3.1318 - val_loss: 3.3210 - val_mae: 3.3210 - val_rmse: 3.8950\n",
      "Epoch 120/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 3.0272 - mae: 3.0272 - rmse: 3.6013 - val_loss: 2.4712 - val_mae: 2.4712 - val_rmse: 2.8917\n",
      "Epoch 121/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.4880 - mae: 2.4880 - rmse: 3.0802 - val_loss: 2.5279 - val_mae: 2.5279 - val_rmse: 2.9517\n",
      "Epoch 122/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.5158 - mae: 2.5158 - rmse: 3.0141 - val_loss: 2.9048 - val_mae: 2.9048 - val_rmse: 3.3447\n",
      "Epoch 123/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.7010 - mae: 2.7010 - rmse: 3.2455 - val_loss: 2.4245 - val_mae: 2.4245 - val_rmse: 2.8517\n",
      "Epoch 124/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.4727 - mae: 2.4727 - rmse: 2.9859 - val_loss: 2.7606 - val_mae: 2.7606 - val_rmse: 3.1737\n",
      "Epoch 125/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 2.6416 - mae: 2.6416 - rmse: 3.1644 - val_loss: 2.6612 - val_mae: 2.6612 - val_rmse: 3.0706\n",
      "Epoch 126/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 2.6065 - mae: 2.6065 - rmse: 3.1240 - val_loss: 2.5091 - val_mae: 2.5091 - val_rmse: 2.9345\n",
      "Epoch 127/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.5428 - mae: 2.5428 - rmse: 3.0547 - val_loss: 2.6639 - val_mae: 2.6639 - val_rmse: 3.0715\n",
      "Epoch 128/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.5059 - mae: 2.5059 - rmse: 3.0631 - val_loss: 2.6970 - val_mae: 2.6970 - val_rmse: 3.1025\n",
      "Epoch 129/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.6586 - mae: 2.6586 - rmse: 3.1804 - val_loss: 2.5810 - val_mae: 2.5810 - val_rmse: 2.9987\n",
      "Epoch 130/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.4513 - mae: 2.4513 - rmse: 3.0130 - val_loss: 2.6584 - val_mae: 2.6584 - val_rmse: 3.0652\n",
      "Epoch 131/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.5575 - mae: 2.5575 - rmse: 3.0679 - val_loss: 2.9518 - val_mae: 2.9518 - val_rmse: 3.4055\n",
      "Epoch 132/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.8046 - mae: 2.8046 - rmse: 3.3579 - val_loss: 2.5567 - val_mae: 2.5567 - val_rmse: 2.9803\n",
      "Epoch 133/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.4864 - mae: 2.4864 - rmse: 3.0591 - val_loss: 2.5539 - val_mae: 2.5539 - val_rmse: 2.9884\n",
      "Epoch 134/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.4526 - mae: 2.4526 - rmse: 2.9611 - val_loss: 2.9894 - val_mae: 2.9894 - val_rmse: 3.4358\n",
      "Epoch 135/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.7624 - mae: 2.7624 - rmse: 3.3104 - val_loss: 2.5698 - val_mae: 2.5698 - val_rmse: 2.9788\n",
      "Epoch 136/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.5472 - mae: 2.5472 - rmse: 3.0846 - val_loss: 2.5543 - val_mae: 2.5543 - val_rmse: 2.9534\n",
      "Epoch 137/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.4987 - mae: 2.4987 - rmse: 3.0246 - val_loss: 2.7648 - val_mae: 2.7648 - val_rmse: 3.1816\n",
      "Epoch 138/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.6553 - mae: 2.6553 - rmse: 3.1891 - val_loss: 2.6473 - val_mae: 2.6473 - val_rmse: 3.0545\n",
      "Epoch 139/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.5520 - mae: 2.5520 - rmse: 3.1034 - val_loss: 2.6217 - val_mae: 2.6217 - val_rmse: 3.0321\n",
      "Epoch 140/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.5333 - mae: 2.5333 - rmse: 3.0460 - val_loss: 2.8431 - val_mae: 2.8431 - val_rmse: 3.2594\n",
      "Epoch 141/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.6779 - mae: 2.6779 - rmse: 3.2174 - val_loss: 2.5532 - val_mae: 2.5532 - val_rmse: 2.9624\n",
      "Epoch 142/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.5885 - mae: 2.5885 - rmse: 3.1223 - val_loss: 2.4653 - val_mae: 2.4653 - val_rmse: 2.8955\n",
      "Epoch 143/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.4300 - mae: 2.4300 - rmse: 2.9563 - val_loss: 2.8124 - val_mae: 2.8124 - val_rmse: 3.2224\n",
      "Epoch 144/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.7111 - mae: 2.7111 - rmse: 3.2518 - val_loss: 2.4133 - val_mae: 2.4133 - val_rmse: 2.8679\n",
      "Epoch 145/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.3806 - mae: 2.3806 - rmse: 2.9166 - val_loss: 2.7638 - val_mae: 2.7638 - val_rmse: 3.1672\n",
      "Epoch 146/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.6371 - mae: 2.6371 - rmse: 3.1748 - val_loss: 2.8501 - val_mae: 2.8501 - val_rmse: 3.2662\n",
      "Epoch 147/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.6953 - mae: 2.6953 - rmse: 3.2465 - val_loss: 2.4975 - val_mae: 2.4975 - val_rmse: 2.9281\n",
      "Epoch 148/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.5131 - mae: 2.5131 - rmse: 3.0429 - val_loss: 2.6809 - val_mae: 2.6809 - val_rmse: 3.0845\n",
      "Epoch 149/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.5566 - mae: 2.5566 - rmse: 3.0957 - val_loss: 2.7325 - val_mae: 2.7325 - val_rmse: 3.1342\n",
      "Epoch 150/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.6152 - mae: 2.6152 - rmse: 3.1500 - val_loss: 2.7387 - val_mae: 2.7387 - val_rmse: 3.1435\n",
      "Epoch 151/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 2.5575 - mae: 2.5575 - rmse: 3.1061 - val_loss: 2.6226 - val_mae: 2.6226 - val_rmse: 3.0330\n",
      "Epoch 152/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.4475 - mae: 2.4475 - rmse: 2.9849 - val_loss: 2.8585 - val_mae: 2.8585 - val_rmse: 3.2710\n",
      "Epoch 153/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.6460 - mae: 2.6460 - rmse: 3.1830 - val_loss: 2.7927 - val_mae: 2.7927 - val_rmse: 3.1982\n",
      "Epoch 154/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.6144 - mae: 2.6144 - rmse: 3.1601 - val_loss: 2.7289 - val_mae: 2.7289 - val_rmse: 3.1301\n",
      "Epoch 155/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.5545 - mae: 2.5545 - rmse: 3.0923 - val_loss: 2.8071 - val_mae: 2.8071 - val_rmse: 3.2110\n",
      "Epoch 156/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.5699 - mae: 2.5699 - rmse: 3.1170 - val_loss: 2.9787 - val_mae: 2.9787 - val_rmse: 3.4028\n",
      "Epoch 157/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.7462 - mae: 2.7462 - rmse: 3.3055 - val_loss: 2.5242 - val_mae: 2.5242 - val_rmse: 2.9577\n",
      "Epoch 158/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 2.5107 - mae: 2.5107 - rmse: 3.0653 - val_loss: 2.7102 - val_mae: 2.7102 - val_rmse: 3.1252\n",
      "Epoch 159/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 2.6037 - mae: 2.6037 - rmse: 3.1522 - val_loss: 2.6254 - val_mae: 2.6254 - val_rmse: 3.0406\n",
      "Epoch 160/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 2.4918 - mae: 2.4918 - rmse: 3.0463 - val_loss: 2.8955 - val_mae: 2.8955 - val_rmse: 3.3044\n",
      "Epoch 161/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 2.6157 - mae: 2.6157 - rmse: 3.1616 - val_loss: 2.8734 - val_mae: 2.8734 - val_rmse: 3.2824\n",
      "Epoch 162/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 2.6276 - mae: 2.6276 - rmse: 3.1781 - val_loss: 2.7156 - val_mae: 2.7156 - val_rmse: 3.1218\n",
      "Epoch 163/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 2.5860 - mae: 2.5860 - rmse: 3.1508 - val_loss: 2.6490 - val_mae: 2.6490 - val_rmse: 3.0742\n",
      "Epoch 164/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.5327 - mae: 2.5327 - rmse: 3.0883 - val_loss: 2.8056 - val_mae: 2.8056 - val_rmse: 3.2083\n",
      "Epoch 165/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.5426 - mae: 2.5426 - rmse: 3.1063 - val_loss: 3.2413 - val_mae: 3.2413 - val_rmse: 3.7498\n",
      "Epoch 166/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.8908 - mae: 2.8908 - rmse: 3.4668 - val_loss: 2.5892 - val_mae: 2.5892 - val_rmse: 3.0212\n",
      "Epoch 167/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.5310 - mae: 2.5310 - rmse: 3.0886 - val_loss: 2.6928 - val_mae: 2.6928 - val_rmse: 3.1346\n",
      "Epoch 168/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.6544 - mae: 2.6544 - rmse: 3.1915 - val_loss: 2.9158 - val_mae: 2.9158 - val_rmse: 3.3349\n",
      "Epoch 169/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.7178 - mae: 2.7178 - rmse: 3.2885 - val_loss: 2.4752 - val_mae: 2.4752 - val_rmse: 3.0340\n",
      "Epoch 170/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.4051 - mae: 2.4051 - rmse: 2.9673 - val_loss: 2.7009 - val_mae: 2.7009 - val_rmse: 3.1231\n",
      "Epoch 171/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.6580 - mae: 2.6580 - rmse: 3.1827 - val_loss: 2.6269 - val_mae: 2.6269 - val_rmse: 3.0517\n",
      "Epoch 172/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.6612 - mae: 2.6612 - rmse: 3.2031 - val_loss: 2.6409 - val_mae: 2.6409 - val_rmse: 3.0519\n",
      "Epoch 173/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.6636 - mae: 2.6636 - rmse: 3.2162 - val_loss: 2.5385 - val_mae: 2.5385 - val_rmse: 2.9670\n",
      "Epoch 174/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.4311 - mae: 2.4311 - rmse: 2.9994 - val_loss: 2.3065 - val_mae: 2.3065 - val_rmse: 2.8169\n",
      "Epoch 175/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.3675 - mae: 2.3675 - rmse: 2.8942 - val_loss: 2.7277 - val_mae: 2.7277 - val_rmse: 3.1715\n",
      "Epoch 176/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.5415 - mae: 2.5415 - rmse: 3.0637 - val_loss: 3.0849 - val_mae: 3.0849 - val_rmse: 3.6010\n",
      "Epoch 177/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.8071 - mae: 2.8071 - rmse: 3.3611 - val_loss: 2.8124 - val_mae: 2.8124 - val_rmse: 3.2545\n",
      "Epoch 178/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.5874 - mae: 2.5874 - rmse: 3.1480 - val_loss: 2.6477 - val_mae: 2.6477 - val_rmse: 3.0842\n",
      "Epoch 179/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.5558 - mae: 2.5558 - rmse: 3.0911 - val_loss: 2.6669 - val_mae: 2.6669 - val_rmse: 3.1131\n",
      "Epoch 180/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.5366 - mae: 2.5366 - rmse: 3.0983 - val_loss: 2.5522 - val_mae: 2.5522 - val_rmse: 2.9969\n",
      "Epoch 181/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.5285 - mae: 2.5285 - rmse: 3.0569 - val_loss: 2.6478 - val_mae: 2.6478 - val_rmse: 3.0880\n",
      "Epoch 182/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.6171 - mae: 2.6171 - rmse: 3.1640 - val_loss: 2.6020 - val_mae: 2.6020 - val_rmse: 3.0386\n",
      "Epoch 183/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.5316 - mae: 2.5316 - rmse: 3.0878 - val_loss: 2.6094 - val_mae: 2.6094 - val_rmse: 3.0527\n",
      "Epoch 184/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.5067 - mae: 2.5067 - rmse: 3.0427 - val_loss: 2.5857 - val_mae: 2.5857 - val_rmse: 3.0323\n",
      "Epoch 185/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.4790 - mae: 2.4790 - rmse: 3.0120 - val_loss: 2.9023 - val_mae: 2.9023 - val_rmse: 3.3797\n",
      "Epoch 186/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.6721 - mae: 2.6721 - rmse: 3.2247 - val_loss: 2.9477 - val_mae: 2.9477 - val_rmse: 3.4392\n",
      "Epoch 187/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.7719 - mae: 2.7719 - rmse: 3.3404 - val_loss: 2.5818 - val_mae: 2.5818 - val_rmse: 3.0367\n",
      "Epoch 188/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 2.5527 - mae: 2.5527 - rmse: 3.1300 - val_loss: 2.4436 - val_mae: 2.4436 - val_rmse: 2.9218\n",
      "Epoch 189/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.3836 - mae: 2.3836 - rmse: 2.9097 - val_loss: 2.4203 - val_mae: 2.4203 - val_rmse: 2.8811\n",
      "Epoch 190/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.4662 - mae: 2.4662 - rmse: 3.0028 - val_loss: 2.9670 - val_mae: 2.9670 - val_rmse: 3.4804\n",
      "Epoch 191/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.7168 - mae: 2.7168 - rmse: 3.2750 - val_loss: 2.7683 - val_mae: 2.7683 - val_rmse: 3.2217\n",
      "Epoch 192/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.6381 - mae: 2.6381 - rmse: 3.2312 - val_loss: 2.4584 - val_mae: 2.4584 - val_rmse: 2.9329\n",
      "Epoch 193/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.4107 - mae: 2.4107 - rmse: 2.9356 - val_loss: 2.4736 - val_mae: 2.4736 - val_rmse: 2.9213\n",
      "Epoch 194/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.4083 - mae: 2.4083 - rmse: 2.9408 - val_loss: 2.8438 - val_mae: 2.8438 - val_rmse: 3.3034\n",
      "Epoch 195/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.5835 - mae: 2.5835 - rmse: 3.1499 - val_loss: 3.0364 - val_mae: 3.0364 - val_rmse: 3.5432\n",
      "Epoch 196/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.8476 - mae: 2.8476 - rmse: 3.4072 - val_loss: 2.6246 - val_mae: 2.6246 - val_rmse: 3.0536\n",
      "Epoch 197/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.5748 - mae: 2.5748 - rmse: 3.1482 - val_loss: 2.5337 - val_mae: 2.5337 - val_rmse: 2.9527\n",
      "Epoch 198/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.4843 - mae: 2.4843 - rmse: 3.0112 - val_loss: 2.9788 - val_mae: 2.9788 - val_rmse: 3.4645\n",
      "Epoch 199/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.7553 - mae: 2.7553 - rmse: 3.3139 - val_loss: 2.7663 - val_mae: 2.7663 - val_rmse: 3.2020\n",
      "Epoch 200/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.6860 - mae: 2.6860 - rmse: 3.2990 - val_loss: 2.2501 - val_mae: 2.2501 - val_rmse: 2.7963\n",
      "Epoch 201/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.3103 - mae: 2.3103 - rmse: 2.8527 - val_loss: 2.2531 - val_mae: 2.2531 - val_rmse: 2.7655\n",
      "Epoch 202/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.3064 - mae: 2.3064 - rmse: 2.8341 - val_loss: 2.3267 - val_mae: 2.3267 - val_rmse: 2.7664\n",
      "Epoch 203/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.3835 - mae: 2.3835 - rmse: 2.9326 - val_loss: 2.8869 - val_mae: 2.8869 - val_rmse: 3.3513\n",
      "Epoch 204/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.7359 - mae: 2.7359 - rmse: 3.2765 - val_loss: 3.0772 - val_mae: 3.0772 - val_rmse: 3.6064\n",
      "Epoch 205/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.9115 - mae: 2.9115 - rmse: 3.4718 - val_loss: 2.8380 - val_mae: 2.8380 - val_rmse: 3.2897\n",
      "Epoch 206/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.6503 - mae: 2.6503 - rmse: 3.2649 - val_loss: 2.3920 - val_mae: 2.3920 - val_rmse: 2.8480\n",
      "Epoch 207/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.3949 - mae: 2.3949 - rmse: 2.9037 - val_loss: 2.5463 - val_mae: 2.5463 - val_rmse: 2.9721\n",
      "Epoch 208/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.5849 - mae: 2.5849 - rmse: 3.1060 - val_loss: 3.0321 - val_mae: 3.0321 - val_rmse: 3.5345\n",
      "Epoch 209/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.8648 - mae: 2.8648 - rmse: 3.4225 - val_loss: 2.6577 - val_mae: 2.6577 - val_rmse: 3.0619\n",
      "Epoch 210/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.5313 - mae: 2.5313 - rmse: 3.0994 - val_loss: 2.7738 - val_mae: 2.7738 - val_rmse: 3.1744\n",
      "Epoch 211/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.5820 - mae: 2.5820 - rmse: 3.1126 - val_loss: 2.9631 - val_mae: 2.9631 - val_rmse: 3.4010\n",
      "Epoch 212/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.8919 - mae: 2.8919 - rmse: 3.4663 - val_loss: 2.8558 - val_mae: 2.8558 - val_rmse: 3.2654\n",
      "Epoch 213/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.6678 - mae: 2.6678 - rmse: 3.2442 - val_loss: 2.4740 - val_mae: 2.4740 - val_rmse: 2.9168\n",
      "Epoch 214/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.3823 - mae: 2.3823 - rmse: 2.9225 - val_loss: 2.6270 - val_mae: 2.6270 - val_rmse: 3.0586\n",
      "Epoch 215/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.4469 - mae: 2.4469 - rmse: 2.9780 - val_loss: 2.6306 - val_mae: 2.6306 - val_rmse: 3.0339\n",
      "Epoch 216/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.5249 - mae: 2.5249 - rmse: 3.0495 - val_loss: 3.0140 - val_mae: 3.0140 - val_rmse: 3.4360\n",
      "Epoch 217/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 2.7624 - mae: 2.7624 - rmse: 3.3226 - val_loss: 2.5994 - val_mae: 2.5994 - val_rmse: 3.0116\n",
      "Epoch 218/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.4669 - mae: 2.4669 - rmse: 3.0232 - val_loss: 2.4829 - val_mae: 2.4829 - val_rmse: 2.9085\n",
      "Epoch 219/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.4542 - mae: 2.4542 - rmse: 2.9902 - val_loss: 2.9263 - val_mae: 2.9263 - val_rmse: 3.4365\n",
      "Epoch 220/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.8347 - mae: 2.8347 - rmse: 3.3730 - val_loss: 2.6466 - val_mae: 2.6466 - val_rmse: 3.0463\n",
      "Epoch 221/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.6146 - mae: 2.6146 - rmse: 3.1745 - val_loss: 2.4548 - val_mae: 2.4548 - val_rmse: 2.8316\n",
      "Epoch 222/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.3707 - mae: 2.3707 - rmse: 2.9458 - val_loss: 2.5070 - val_mae: 2.5070 - val_rmse: 2.9695\n",
      "Epoch 223/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.3952 - mae: 2.3952 - rmse: 2.9556 - val_loss: 3.0027 - val_mae: 3.0027 - val_rmse: 3.4255\n",
      "Epoch 224/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.6861 - mae: 2.6861 - rmse: 3.2403 - val_loss: 3.0591 - val_mae: 3.0591 - val_rmse: 3.4945\n",
      "Epoch 225/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.7726 - mae: 2.7726 - rmse: 3.3545 - val_loss: 2.4233 - val_mae: 2.4233 - val_rmse: 2.9392\n",
      "Epoch 226/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.5115 - mae: 2.5115 - rmse: 3.1233 - val_loss: 2.2455 - val_mae: 2.2455 - val_rmse: 2.7931\n",
      "Epoch 227/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.3388 - mae: 2.3388 - rmse: 2.9144 - val_loss: 2.3092 - val_mae: 2.3092 - val_rmse: 2.8190\n",
      "Epoch 228/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.3047 - mae: 2.3047 - rmse: 2.8576 - val_loss: 2.5960 - val_mae: 2.5960 - val_rmse: 3.0365\n",
      "Epoch 229/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.5099 - mae: 2.5099 - rmse: 3.0725 - val_loss: 2.7615 - val_mae: 2.7615 - val_rmse: 3.1751\n",
      "Epoch 230/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.6609 - mae: 2.6609 - rmse: 3.2077 - val_loss: 2.8552 - val_mae: 2.8552 - val_rmse: 3.3277\n",
      "Epoch 231/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.9055 - mae: 2.9055 - rmse: 3.4771 - val_loss: 2.7308 - val_mae: 2.7308 - val_rmse: 3.1323\n",
      "Epoch 232/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.6417 - mae: 2.6417 - rmse: 3.2011 - val_loss: 2.5008 - val_mae: 2.5008 - val_rmse: 2.9469\n",
      "Epoch 233/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.4748 - mae: 2.4748 - rmse: 3.0484 - val_loss: 2.5730 - val_mae: 2.5730 - val_rmse: 3.0203\n",
      "Epoch 234/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.4709 - mae: 2.4709 - rmse: 3.0541 - val_loss: 2.6763 - val_mae: 2.6763 - val_rmse: 3.1089\n",
      "Epoch 235/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.5721 - mae: 2.5721 - rmse: 3.1652 - val_loss: 2.5864 - val_mae: 2.5864 - val_rmse: 3.0668\n",
      "Epoch 236/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.4916 - mae: 2.4916 - rmse: 3.0835 - val_loss: 2.3093 - val_mae: 2.3093 - val_rmse: 2.8409\n",
      "Epoch 237/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.3290 - mae: 2.3290 - rmse: 2.9070 - val_loss: 2.3182 - val_mae: 2.3182 - val_rmse: 2.8344\n",
      "Epoch 238/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.3172 - mae: 2.3172 - rmse: 2.8920 - val_loss: 2.5731 - val_mae: 2.5731 - val_rmse: 2.9982\n",
      "Epoch 239/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.4457 - mae: 2.4457 - rmse: 3.0313 - val_loss: 3.1425 - val_mae: 3.1425 - val_rmse: 3.6041\n",
      "Epoch 240/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.8387 - mae: 2.8387 - rmse: 3.4104 - val_loss: 2.8582 - val_mae: 2.8582 - val_rmse: 3.2874\n",
      "Epoch 241/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.7706 - mae: 2.7706 - rmse: 3.4165 - val_loss: 2.5150 - val_mae: 2.5150 - val_rmse: 3.0051\n",
      "Epoch 242/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 2.6329 - mae: 2.6329 - rmse: 3.2760 - val_loss: 2.3512 - val_mae: 2.3512 - val_rmse: 2.8642\n",
      "Epoch 243/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.4521 - mae: 2.4521 - rmse: 3.0485 - val_loss: 2.4689 - val_mae: 2.4689 - val_rmse: 2.9410\n",
      "Epoch 244/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.3703 - mae: 2.3703 - rmse: 2.9506 - val_loss: 3.0511 - val_mae: 3.0511 - val_rmse: 3.4882\n",
      "Epoch 245/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.7505 - mae: 2.7505 - rmse: 3.3259 - val_loss: 2.7646 - val_mae: 2.7646 - val_rmse: 3.1803\n",
      "Epoch 246/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.5732 - mae: 2.5732 - rmse: 3.1675 - val_loss: 2.7979 - val_mae: 2.7979 - val_rmse: 3.2598\n",
      "Epoch 247/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.6641 - mae: 2.6641 - rmse: 3.2838 - val_loss: 2.2980 - val_mae: 2.2980 - val_rmse: 2.8462\n",
      "Epoch 248/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.4502 - mae: 2.4502 - rmse: 3.0652 - val_loss: 2.2868 - val_mae: 2.2868 - val_rmse: 2.8822\n",
      "Epoch 249/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 2.3332 - mae: 2.3332 - rmse: 2.9096 - val_loss: 2.5780 - val_mae: 2.5780 - val_rmse: 2.9985\n",
      "Epoch 250/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.5001 - mae: 2.5001 - rmse: 3.0574 - val_loss: 3.0807 - val_mae: 3.0807 - val_rmse: 3.5417\n",
      "Epoch 251/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.7876 - mae: 2.7876 - rmse: 3.3731 - val_loss: 2.7528 - val_mae: 2.7528 - val_rmse: 3.1654\n",
      "Epoch 252/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.5955 - mae: 2.5955 - rmse: 3.1350 - val_loss: 2.6580 - val_mae: 2.6580 - val_rmse: 3.1753\n",
      "Epoch 253/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.6299 - mae: 2.6299 - rmse: 3.2185 - val_loss: 2.5939 - val_mae: 2.5939 - val_rmse: 3.0266\n",
      "Epoch 254/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.6804 - mae: 2.6804 - rmse: 3.2195 - val_loss: 2.4607 - val_mae: 2.4607 - val_rmse: 2.9146\n",
      "Epoch 255/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.4946 - mae: 2.4946 - rmse: 3.0665 - val_loss: 2.5570 - val_mae: 2.5570 - val_rmse: 3.0777\n",
      "Epoch 256/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.3842 - mae: 2.3842 - rmse: 2.9557 - val_loss: 2.4361 - val_mae: 2.4361 - val_rmse: 2.8982\n",
      "Epoch 257/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.4942 - mae: 2.4942 - rmse: 3.0400 - val_loss: 2.9363 - val_mae: 2.9363 - val_rmse: 3.4130\n",
      "Epoch 258/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 2.7320 - mae: 2.7320 - rmse: 3.3161 - val_loss: 2.4790 - val_mae: 2.4790 - val_rmse: 2.9243\n",
      "Epoch 259/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 2.4142 - mae: 2.4142 - rmse: 2.9635 - val_loss: 2.4404 - val_mae: 2.4404 - val_rmse: 2.9501\n",
      "Epoch 260/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.3874 - mae: 2.3874 - rmse: 2.9588 - val_loss: 2.7448 - val_mae: 2.7448 - val_rmse: 3.2024\n",
      "Epoch 261/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.5454 - mae: 2.5454 - rmse: 3.0954 - val_loss: 2.5126 - val_mae: 2.5126 - val_rmse: 2.9244\n",
      "Epoch 262/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.5153 - mae: 2.5153 - rmse: 3.0597 - val_loss: 3.0250 - val_mae: 3.0250 - val_rmse: 3.4634\n",
      "Epoch 263/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.8118 - mae: 2.8118 - rmse: 3.3932 - val_loss: 2.9013 - val_mae: 2.9013 - val_rmse: 3.3207\n",
      "Epoch 264/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.6357 - mae: 2.6357 - rmse: 3.2270 - val_loss: 2.4112 - val_mae: 2.4112 - val_rmse: 2.9311\n",
      "Epoch 265/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.5151 - mae: 2.5151 - rmse: 3.1245 - val_loss: 2.1976 - val_mae: 2.1976 - val_rmse: 2.8523\n",
      "Epoch 266/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.3290 - mae: 2.3290 - rmse: 2.9492 - val_loss: 2.2564 - val_mae: 2.2564 - val_rmse: 2.9420\n",
      "Epoch 267/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.2747 - mae: 2.2747 - rmse: 2.8135 - val_loss: 2.3906 - val_mae: 2.3906 - val_rmse: 2.8554\n",
      "Epoch 268/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 2.4724 - mae: 2.4724 - rmse: 3.0494 - val_loss: 2.8680 - val_mae: 2.8680 - val_rmse: 3.2820\n",
      "Epoch 269/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.8212 - mae: 2.8212 - rmse: 3.3926 - val_loss: 2.7023 - val_mae: 2.7023 - val_rmse: 3.1024\n",
      "Epoch 270/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.6680 - mae: 2.6680 - rmse: 3.2244 - val_loss: 2.3215 - val_mae: 2.3215 - val_rmse: 2.8915\n",
      "Epoch 271/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.4615 - mae: 2.4615 - rmse: 3.0862 - val_loss: 2.2030 - val_mae: 2.2030 - val_rmse: 2.8587\n",
      "Epoch 272/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.3221 - mae: 2.3221 - rmse: 2.9216 - val_loss: 2.2028 - val_mae: 2.2028 - val_rmse: 2.8371\n",
      "Epoch 273/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.3022 - mae: 2.3022 - rmse: 2.8903 - val_loss: 2.2205 - val_mae: 2.2205 - val_rmse: 2.8489\n",
      "Epoch 274/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.3443 - mae: 2.3443 - rmse: 2.8865 - val_loss: 2.5956 - val_mae: 2.5956 - val_rmse: 3.0214\n",
      "Epoch 275/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.6927 - mae: 2.6927 - rmse: 3.2495 - val_loss: 2.8898 - val_mae: 2.8898 - val_rmse: 3.3057\n",
      "Epoch 276/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.9581 - mae: 2.9581 - rmse: 3.5210 - val_loss: 2.3762 - val_mae: 2.3762 - val_rmse: 2.7773\n",
      "Epoch 277/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.6606 - mae: 2.6606 - rmse: 3.3555 - val_loss: 2.4561 - val_mae: 2.4561 - val_rmse: 3.1387\n",
      "Epoch 278/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.6204 - mae: 2.6204 - rmse: 3.2611 - val_loss: 2.4409 - val_mae: 2.4409 - val_rmse: 3.1409\n",
      "Epoch 279/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.4189 - mae: 2.4189 - rmse: 3.0187 - val_loss: 2.3127 - val_mae: 2.3127 - val_rmse: 2.9697\n",
      "Epoch 280/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.3270 - mae: 2.3270 - rmse: 2.8598 - val_loss: 2.1940 - val_mae: 2.1940 - val_rmse: 2.8320\n",
      "Epoch 281/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.4243 - mae: 2.4243 - rmse: 2.9922 - val_loss: 3.1984 - val_mae: 3.1984 - val_rmse: 3.7271\n",
      "Epoch 282/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.9101 - mae: 2.9101 - rmse: 3.4722 - val_loss: 2.4869 - val_mae: 2.4869 - val_rmse: 2.9438\n",
      "Epoch 283/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.4467 - mae: 2.4467 - rmse: 3.0457 - val_loss: 2.2142 - val_mae: 2.2142 - val_rmse: 2.7502\n",
      "Epoch 284/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.2744 - mae: 2.2744 - rmse: 2.8142 - val_loss: 2.2333 - val_mae: 2.2333 - val_rmse: 2.7213\n",
      "Epoch 285/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.2966 - mae: 2.2966 - rmse: 2.8151 - val_loss: 2.6531 - val_mae: 2.6531 - val_rmse: 3.0467\n",
      "Epoch 286/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.5653 - mae: 2.5653 - rmse: 3.1481 - val_loss: 3.2573 - val_mae: 3.2573 - val_rmse: 3.7896\n",
      "Epoch 287/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 3.0008 - mae: 3.0008 - rmse: 3.5643 - val_loss: 2.5398 - val_mae: 2.5398 - val_rmse: 2.9565\n",
      "Epoch 288/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 2.4741 - mae: 2.4741 - rmse: 3.0108 - val_loss: 2.4109 - val_mae: 2.4109 - val_rmse: 2.8869\n",
      "Epoch 289/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.4450 - mae: 2.4450 - rmse: 2.9901 - val_loss: 2.3962 - val_mae: 2.3962 - val_rmse: 2.8856\n",
      "Epoch 290/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.4318 - mae: 2.4318 - rmse: 2.9949 - val_loss: 2.2479 - val_mae: 2.2479 - val_rmse: 2.8057\n",
      "Epoch 291/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.3279 - mae: 2.3279 - rmse: 2.8880 - val_loss: 2.2627 - val_mae: 2.2627 - val_rmse: 2.7384\n",
      "Epoch 292/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.2816 - mae: 2.2816 - rmse: 2.8215 - val_loss: 2.5829 - val_mae: 2.5829 - val_rmse: 2.9969\n",
      "Epoch 293/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.5733 - mae: 2.5733 - rmse: 3.1756 - val_loss: 3.2083 - val_mae: 3.2083 - val_rmse: 3.7148\n",
      "Epoch 294/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 3.1087 - mae: 3.1087 - rmse: 3.6733 - val_loss: 2.3009 - val_mae: 2.3009 - val_rmse: 2.7810\n",
      "Epoch 295/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.7789 - mae: 2.7789 - rmse: 3.5421 - val_loss: 3.0944 - val_mae: 3.0944 - val_rmse: 3.8371\n",
      "Epoch 296/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.7616 - mae: 2.7616 - rmse: 3.4569 - val_loss: 2.7737 - val_mae: 2.7737 - val_rmse: 3.5564\n",
      "Epoch 297/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.5524 - mae: 2.5524 - rmse: 3.1455 - val_loss: 2.5475 - val_mae: 2.5475 - val_rmse: 3.1934\n",
      "Epoch 298/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.6059 - mae: 2.6059 - rmse: 3.2390 - val_loss: 3.4397 - val_mae: 3.4397 - val_rmse: 4.1147\n",
      "Epoch 299/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 3.4156 - mae: 3.4156 - rmse: 4.0057 - val_loss: 2.2776 - val_mae: 2.2776 - val_rmse: 2.8372\n",
      "Epoch 300/300\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.5757 - mae: 2.5757 - rmse: 3.2953 - val_loss: 2.2656 - val_mae: 2.2656 - val_rmse: 2.9117\n",
      "Test MAE=2.613  RMSE=3.069  R²=0.823\n"
     ]
    }
   ],
   "source": [
    "nn_model = keras.Sequential([\n",
    "    keras.layers.Input(shape=(Xtr.shape[1],)),    # input size = number of features\n",
    "    keras.layers.Dense(64, activation=\"relu\"),\n",
    "    keras.layers.Dense(32, activation=\"relu\"),\n",
    "    keras.layers.Dense(16, activation=\"relu\"),\n",
    "    keras.layers.Dense(1, activation=\"linear\")    # regression output\n",
    "])\n",
    "\n",
    "nn_model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.1),\n",
    "    loss=\"mae\",\n",
    "    metrics=[tf.keras.metrics.RootMeanSquaredError(name=\"rmse\"), \"mae\"]\n",
    ")\n",
    "\n",
    "history = nn_model.fit(\n",
    "    Xtr, ytr,\n",
    "    validation_split=0.2,   # 20% of training data for validation\n",
    "    epochs=300,       \n",
    "    batch_size=32,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate test metrics\n",
    "test_loss, test_rmse, test_mae = nn_model.evaluate(Xte, yte, verbose=0)\n",
    "\n",
    "# Predict test set\n",
    "pred = nn_model.predict(Xte, verbose=0).ravel()\n",
    "\n",
    "# Compute R²\n",
    "r2 = r2_score(yte, pred)\n",
    "\n",
    "print(f\"Test MAE={test_mae:.3f}  RMSE={test_rmse:.3f}  R²={r2:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimenting with hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test with learning rates 0.001\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - loss: 2.2944 - mae: 2.2944 - rmse: 2.8117 - val_loss: 2.1819 - val_mae: 2.1819 - val_rmse: 2.7680\n",
      "Epoch 2/150\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.2675 - mae: 2.2675 - rmse: 2.7978 - val_loss: 2.2127 - val_mae: 2.2127 - val_rmse: 2.7674\n",
      "Epoch 3/150\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.2726 - mae: 2.2726 - rmse: 2.8049 - val_loss: 2.1899 - val_mae: 2.1899 - val_rmse: 2.7665\n",
      "Epoch 4/150\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.2681 - mae: 2.2681 - rmse: 2.7923 - val_loss: 2.1840 - val_mae: 2.1840 - val_rmse: 2.7708\n",
      "Epoch 5/150\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.2656 - mae: 2.2656 - rmse: 2.7872 - val_loss: 2.1860 - val_mae: 2.1860 - val_rmse: 2.7709\n",
      "Epoch 6/150\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.2640 - mae: 2.2640 - rmse: 2.7856 - val_loss: 2.1884 - val_mae: 2.1884 - val_rmse: 2.7653\n",
      "Epoch 7/150\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.2618 - mae: 2.2618 - rmse: 2.7852 - val_loss: 2.1930 - val_mae: 2.1930 - val_rmse: 2.7575\n",
      "Epoch 8/150\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.2624 - mae: 2.2624 - rmse: 2.7880 - val_loss: 2.1931 - val_mae: 2.1931 - val_rmse: 2.7566\n",
      "Epoch 9/150\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.2616 - mae: 2.2616 - rmse: 2.7865 - val_loss: 2.1936 - val_mae: 2.1936 - val_rmse: 2.7568\n",
      "Epoch 10/150\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.2605 - mae: 2.2605 - rmse: 2.7853 - val_loss: 2.1941 - val_mae: 2.1941 - val_rmse: 2.7561\n",
      "Epoch 11/150\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.2602 - mae: 2.2602 - rmse: 2.7848 - val_loss: 2.1955 - val_mae: 2.1955 - val_rmse: 2.7534\n",
      "Epoch 12/150\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.2592 - mae: 2.2592 - rmse: 2.7863 - val_loss: 2.2010 - val_mae: 2.2010 - val_rmse: 2.7498\n",
      "Epoch 13/150\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.2608 - mae: 2.2608 - rmse: 2.7888 - val_loss: 2.1972 - val_mae: 2.1972 - val_rmse: 2.7494\n",
      "Epoch 14/150\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.2611 - mae: 2.2611 - rmse: 2.7860 - val_loss: 2.1940 - val_mae: 2.1940 - val_rmse: 2.7534\n",
      "Epoch 15/150\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.2593 - mae: 2.2593 - rmse: 2.7833 - val_loss: 2.1945 - val_mae: 2.1945 - val_rmse: 2.7511\n",
      "Epoch 16/150\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.2574 - mae: 2.2574 - rmse: 2.7849 - val_loss: 2.2005 - val_mae: 2.2005 - val_rmse: 2.7489\n",
      "Epoch 17/150\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.2594 - mae: 2.2594 - rmse: 2.7878 - val_loss: 2.1963 - val_mae: 2.1963 - val_rmse: 2.7497\n",
      "Epoch 18/150\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.2599 - mae: 2.2599 - rmse: 2.7852 - val_loss: 2.1937 - val_mae: 2.1937 - val_rmse: 2.7530\n",
      "Epoch 19/150\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.2576 - mae: 2.2576 - rmse: 2.7828 - val_loss: 2.1946 - val_mae: 2.1946 - val_rmse: 2.7495\n",
      "Epoch 20/150\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.2563 - mae: 2.2563 - rmse: 2.7839 - val_loss: 2.1974 - val_mae: 2.1974 - val_rmse: 2.7480\n",
      "Epoch 21/150\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.2572 - mae: 2.2572 - rmse: 2.7854 - val_loss: 2.1969 - val_mae: 2.1969 - val_rmse: 2.7485\n",
      "Epoch 22/150\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 2.2574 - mae: 2.2574 - rmse: 2.7849 - val_loss: 2.1943 - val_mae: 2.1943 - val_rmse: 2.7504\n",
      "Epoch 23/150\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.2579 - mae: 2.2579 - rmse: 2.7838 - val_loss: 2.1943 - val_mae: 2.1943 - val_rmse: 2.7534\n",
      "Epoch 24/150\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.2555 - mae: 2.2555 - rmse: 2.7822 - val_loss: 2.1952 - val_mae: 2.1952 - val_rmse: 2.7506\n",
      "Epoch 25/150\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.2558 - mae: 2.2558 - rmse: 2.7836 - val_loss: 2.1955 - val_mae: 2.1955 - val_rmse: 2.7502\n",
      "Epoch 26/150\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.2564 - mae: 2.2564 - rmse: 2.7837 - val_loss: 2.1949 - val_mae: 2.1949 - val_rmse: 2.7509\n",
      "Epoch 27/150\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.2554 - mae: 2.2554 - rmse: 2.7832 - val_loss: 2.1959 - val_mae: 2.1959 - val_rmse: 2.7503\n",
      "Epoch 28/150\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.2549 - mae: 2.2549 - rmse: 2.7837 - val_loss: 2.1975 - val_mae: 2.1975 - val_rmse: 2.7493\n",
      "Epoch 29/150\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.2556 - mae: 2.2556 - rmse: 2.7844 - val_loss: 2.1962 - val_mae: 2.1962 - val_rmse: 2.7507\n",
      "Epoch 30/150\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.2558 - mae: 2.2558 - rmse: 2.7836 - val_loss: 2.1957 - val_mae: 2.1957 - val_rmse: 2.7519\n",
      "Epoch 31/150\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 2.2548 - mae: 2.2548 - rmse: 2.7828 - val_loss: 2.1959 - val_mae: 2.1959 - val_rmse: 2.7515\n",
      "Epoch 32/150\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.2543 - mae: 2.2543 - rmse: 2.7829 - val_loss: 2.1962 - val_mae: 2.1962 - val_rmse: 2.7526\n",
      "Epoch 33/150\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.2547 - mae: 2.2547 - rmse: 2.7833 - val_loss: 2.1964 - val_mae: 2.1964 - val_rmse: 2.7537\n",
      "Epoch 34/150\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.2539 - mae: 2.2539 - rmse: 2.7830 - val_loss: 2.1967 - val_mae: 2.1967 - val_rmse: 2.7537\n",
      "Epoch 35/150\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.2544 - mae: 2.2544 - rmse: 2.7832 - val_loss: 2.1969 - val_mae: 2.1969 - val_rmse: 2.7542\n",
      "Epoch 36/150\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.2534 - mae: 2.2534 - rmse: 2.7827 - val_loss: 2.1973 - val_mae: 2.1973 - val_rmse: 2.7542\n",
      "Epoch 37/150\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.2531 - mae: 2.2531 - rmse: 2.7831 - val_loss: 2.1999 - val_mae: 2.1999 - val_rmse: 2.7543\n",
      "Epoch 38/150\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.2535 - mae: 2.2535 - rmse: 2.7836 - val_loss: 2.1988 - val_mae: 2.1988 - val_rmse: 2.7554\n",
      "Epoch 39/150\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.2538 - mae: 2.2538 - rmse: 2.7831 - val_loss: 2.1984 - val_mae: 2.1984 - val_rmse: 2.7573\n",
      "Epoch 40/150\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.2527 - mae: 2.2527 - rmse: 2.7822 - val_loss: 2.1988 - val_mae: 2.1988 - val_rmse: 2.7573\n",
      "Epoch 41/150\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.2518 - mae: 2.2518 - rmse: 2.7822 - val_loss: 2.2004 - val_mae: 2.2004 - val_rmse: 2.7575\n",
      "Epoch 42/150\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.2524 - mae: 2.2524 - rmse: 2.7825 - val_loss: 2.2006 - val_mae: 2.2006 - val_rmse: 2.7583\n",
      "Epoch 43/150\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.2514 - mae: 2.2514 - rmse: 2.7822 - val_loss: 2.2023 - val_mae: 2.2023 - val_rmse: 2.7593\n",
      "Epoch 44/150\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.2520 - mae: 2.2520 - rmse: 2.7824 - val_loss: 2.2024 - val_mae: 2.2024 - val_rmse: 2.7611\n",
      "Epoch 45/150\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.2510 - mae: 2.2510 - rmse: 2.7819 - val_loss: 2.2052 - val_mae: 2.2052 - val_rmse: 2.7617\n",
      "Epoch 46/150\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.2503 - mae: 2.2503 - rmse: 2.7820 - val_loss: 2.2084 - val_mae: 2.2084 - val_rmse: 2.7625\n",
      "Epoch 47/150\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.2507 - mae: 2.2507 - rmse: 2.7824 - val_loss: 2.2083 - val_mae: 2.2083 - val_rmse: 2.7651\n",
      "Epoch 48/150\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.2510 - mae: 2.2510 - rmse: 2.7820 - val_loss: 2.2071 - val_mae: 2.2071 - val_rmse: 2.7681\n",
      "Epoch 49/150\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.2499 - mae: 2.2499 - rmse: 2.7814 - val_loss: 2.2089 - val_mae: 2.2089 - val_rmse: 2.7690\n",
      "Epoch 50/150\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.2489 - mae: 2.2489 - rmse: 2.7813 - val_loss: 2.2127 - val_mae: 2.2127 - val_rmse: 2.7685\n",
      "Epoch 51/150\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.2483 - mae: 2.2483 - rmse: 2.7817 - val_loss: 2.2152 - val_mae: 2.2152 - val_rmse: 2.7670\n",
      "Epoch 52/150\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.2488 - mae: 2.2488 - rmse: 2.7823 - val_loss: 2.2139 - val_mae: 2.2139 - val_rmse: 2.7669\n",
      "Epoch 53/150\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.2488 - mae: 2.2488 - rmse: 2.7819 - val_loss: 2.2119 - val_mae: 2.2119 - val_rmse: 2.7677\n",
      "Epoch 54/150\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.2479 - mae: 2.2479 - rmse: 2.7813 - val_loss: 2.2121 - val_mae: 2.2121 - val_rmse: 2.7689\n",
      "Epoch 55/150\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.2484 - mae: 2.2484 - rmse: 2.7814 - val_loss: 2.2116 - val_mae: 2.2116 - val_rmse: 2.7712\n",
      "Epoch 56/150\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.2473 - mae: 2.2473 - rmse: 2.7810 - val_loss: 2.2139 - val_mae: 2.2139 - val_rmse: 2.7717\n",
      "Epoch 57/150\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.2466 - mae: 2.2466 - rmse: 2.7811 - val_loss: 2.2164 - val_mae: 2.2164 - val_rmse: 2.7719\n",
      "Epoch 58/150\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.2469 - mae: 2.2469 - rmse: 2.7816 - val_loss: 2.2153 - val_mae: 2.2153 - val_rmse: 2.7736\n",
      "Epoch 59/150\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.2472 - mae: 2.2472 - rmse: 2.7814 - val_loss: 2.2136 - val_mae: 2.2136 - val_rmse: 2.7759\n",
      "Epoch 60/150\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.2460 - mae: 2.2460 - rmse: 2.7807 - val_loss: 2.2142 - val_mae: 2.2142 - val_rmse: 2.7751\n",
      "Epoch 61/150\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.2452 - mae: 2.2452 - rmse: 2.7807 - val_loss: 2.2164 - val_mae: 2.2164 - val_rmse: 2.7756\n",
      "Epoch 62/150\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.2457 - mae: 2.2457 - rmse: 2.7815 - val_loss: 2.2167 - val_mae: 2.2167 - val_rmse: 2.7796\n",
      "Epoch 63/150\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.2460 - mae: 2.2460 - rmse: 2.7817 - val_loss: 2.2156 - val_mae: 2.2156 - val_rmse: 2.7836\n",
      "Epoch 64/150\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.2430 - mae: 2.2430 - rmse: 2.7802 - val_loss: 2.2230 - val_mae: 2.2230 - val_rmse: 2.7785\n",
      "Epoch 65/150\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.2440 - mae: 2.2440 - rmse: 2.7820 - val_loss: 2.2252 - val_mae: 2.2252 - val_rmse: 2.7776\n",
      "Epoch 66/150\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.2444 - mae: 2.2444 - rmse: 2.7827 - val_loss: 2.2246 - val_mae: 2.2246 - val_rmse: 2.7788\n",
      "Epoch 67/150\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.2443 - mae: 2.2443 - rmse: 2.7824 - val_loss: 2.2227 - val_mae: 2.2227 - val_rmse: 2.7829\n",
      "Epoch 68/150\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.2442 - mae: 2.2442 - rmse: 2.7821 - val_loss: 2.2205 - val_mae: 2.2205 - val_rmse: 2.7880\n",
      "Epoch 69/150\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.2416 - mae: 2.2416 - rmse: 2.7806 - val_loss: 2.2275 - val_mae: 2.2275 - val_rmse: 2.7838\n",
      "Epoch 70/150\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.2420 - mae: 2.2420 - rmse: 2.7823 - val_loss: 2.2300 - val_mae: 2.2300 - val_rmse: 2.7843\n",
      "Epoch 71/150\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.2424 - mae: 2.2424 - rmse: 2.7833 - val_loss: 2.2299 - val_mae: 2.2299 - val_rmse: 2.7877\n",
      "Epoch 72/150\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.2421 - mae: 2.2421 - rmse: 2.7830 - val_loss: 2.2273 - val_mae: 2.2273 - val_rmse: 2.7919\n",
      "Epoch 73/150\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.2409 - mae: 2.2409 - rmse: 2.7822 - val_loss: 2.2317 - val_mae: 2.2317 - val_rmse: 2.7924\n",
      "Epoch 74/150\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.2394 - mae: 2.2394 - rmse: 2.7832 - val_loss: 2.2407 - val_mae: 2.2407 - val_rmse: 2.7921\n",
      "Epoch 75/150\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.2398 - mae: 2.2398 - rmse: 2.7870 - val_loss: 2.2459 - val_mae: 2.2459 - val_rmse: 2.7979\n",
      "Epoch 76/150\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.2393 - mae: 2.2393 - rmse: 2.7889 - val_loss: 2.2485 - val_mae: 2.2485 - val_rmse: 2.8063\n",
      "Epoch 77/150\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.2373 - mae: 2.2373 - rmse: 2.7898 - val_loss: 2.2522 - val_mae: 2.2522 - val_rmse: 2.8096\n",
      "Epoch 78/150\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.2365 - mae: 2.2365 - rmse: 2.7907 - val_loss: 2.2537 - val_mae: 2.2537 - val_rmse: 2.8088\n",
      "Epoch 79/150\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.2355 - mae: 2.2355 - rmse: 2.7904 - val_loss: 2.2578 - val_mae: 2.2578 - val_rmse: 2.8072\n",
      "Epoch 80/150\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.2356 - mae: 2.2356 - rmse: 2.7922 - val_loss: 2.2581 - val_mae: 2.2581 - val_rmse: 2.8071\n",
      "Epoch 81/150\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.2344 - mae: 2.2344 - rmse: 2.7929 - val_loss: 2.2608 - val_mae: 2.2608 - val_rmse: 2.8062\n",
      "Epoch 82/150\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.2351 - mae: 2.2351 - rmse: 2.7926 - val_loss: 2.2576 - val_mae: 2.2576 - val_rmse: 2.8066\n",
      "Epoch 83/150\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.2339 - mae: 2.2339 - rmse: 2.7907 - val_loss: 2.2611 - val_mae: 2.2611 - val_rmse: 2.8055\n",
      "Epoch 84/150\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.2327 - mae: 2.2327 - rmse: 2.7936 - val_loss: 2.2649 - val_mae: 2.2649 - val_rmse: 2.8055\n",
      "Epoch 85/150\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.2343 - mae: 2.2343 - rmse: 2.7947 - val_loss: 2.2616 - val_mae: 2.2616 - val_rmse: 2.8075\n",
      "Epoch 86/150\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.2327 - mae: 2.2327 - rmse: 2.7932 - val_loss: 2.2651 - val_mae: 2.2651 - val_rmse: 2.8082\n",
      "Epoch 87/150\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.2327 - mae: 2.2327 - rmse: 2.7973 - val_loss: 2.2656 - val_mae: 2.2656 - val_rmse: 2.8086\n",
      "Epoch 88/150\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.2318 - mae: 2.2318 - rmse: 2.7949 - val_loss: 2.2655 - val_mae: 2.2655 - val_rmse: 2.8087\n",
      "Epoch 89/150\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.2322 - mae: 2.2322 - rmse: 2.7964 - val_loss: 2.2637 - val_mae: 2.2637 - val_rmse: 2.8091\n",
      "Epoch 90/150\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.2321 - mae: 2.2321 - rmse: 2.7932 - val_loss: 2.2621 - val_mae: 2.2621 - val_rmse: 2.8098\n",
      "Epoch 91/150\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.2308 - mae: 2.2308 - rmse: 2.7928 - val_loss: 2.2669 - val_mae: 2.2669 - val_rmse: 2.8096\n",
      "Epoch 92/150\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.2318 - mae: 2.2318 - rmse: 2.7981 - val_loss: 2.2665 - val_mae: 2.2665 - val_rmse: 2.8095\n",
      "Epoch 93/150\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.2314 - mae: 2.2314 - rmse: 2.7943 - val_loss: 2.2620 - val_mae: 2.2620 - val_rmse: 2.8096\n",
      "Epoch 94/150\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.2305 - mae: 2.2305 - rmse: 2.7917 - val_loss: 2.2648 - val_mae: 2.2648 - val_rmse: 2.8088\n",
      "Epoch 95/150\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.2304 - mae: 2.2304 - rmse: 2.7950 - val_loss: 2.2682 - val_mae: 2.2682 - val_rmse: 2.8096\n",
      "Epoch 96/150\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.2303 - mae: 2.2303 - rmse: 2.7974 - val_loss: 2.2666 - val_mae: 2.2666 - val_rmse: 2.8107\n",
      "Epoch 97/150\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.2299 - mae: 2.2299 - rmse: 2.7941 - val_loss: 2.2637 - val_mae: 2.2637 - val_rmse: 2.8112\n",
      "Epoch 98/150\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.2295 - mae: 2.2295 - rmse: 2.7933 - val_loss: 2.2633 - val_mae: 2.2633 - val_rmse: 2.8122\n",
      "Epoch 99/150\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.2293 - mae: 2.2293 - rmse: 2.7927 - val_loss: 2.2653 - val_mae: 2.2653 - val_rmse: 2.8133\n",
      "Epoch 100/150\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.2290 - mae: 2.2290 - rmse: 2.7955 - val_loss: 2.2663 - val_mae: 2.2663 - val_rmse: 2.8142\n",
      "Epoch 101/150\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.2288 - mae: 2.2288 - rmse: 2.7944 - val_loss: 2.2653 - val_mae: 2.2653 - val_rmse: 2.8149\n",
      "Epoch 102/150\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.2286 - mae: 2.2286 - rmse: 2.7951 - val_loss: 2.2645 - val_mae: 2.2645 - val_rmse: 2.8160\n",
      "Epoch 103/150\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.2286 - mae: 2.2286 - rmse: 2.7927 - val_loss: 2.2622 - val_mae: 2.2622 - val_rmse: 2.8168\n",
      "Epoch 104/150\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.2278 - mae: 2.2278 - rmse: 2.7922 - val_loss: 2.2650 - val_mae: 2.2650 - val_rmse: 2.8167\n",
      "Epoch 105/150\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.2276 - mae: 2.2276 - rmse: 2.7940 - val_loss: 2.2624 - val_mae: 2.2624 - val_rmse: 2.8167\n",
      "Epoch 106/150\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 2.2282 - mae: 2.2282 - rmse: 2.7903 - val_loss: 2.2590 - val_mae: 2.2590 - val_rmse: 2.8175\n",
      "Epoch 107/150\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.2271 - mae: 2.2271 - rmse: 2.7885 - val_loss: 2.2630 - val_mae: 2.2630 - val_rmse: 2.8168\n",
      "Epoch 108/150\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.2265 - mae: 2.2265 - rmse: 2.7918 - val_loss: 2.2654 - val_mae: 2.2654 - val_rmse: 2.8173\n",
      "Epoch 109/150\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.2263 - mae: 2.2263 - rmse: 2.7916 - val_loss: 2.2606 - val_mae: 2.2606 - val_rmse: 2.8188\n",
      "Epoch 110/150\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.2261 - mae: 2.2261 - rmse: 2.7877 - val_loss: 2.2598 - val_mae: 2.2598 - val_rmse: 2.8202\n",
      "Epoch 111/150\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.2248 - mae: 2.2248 - rmse: 2.7876 - val_loss: 2.2637 - val_mae: 2.2637 - val_rmse: 2.8202\n",
      "Epoch 112/150\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.2248 - mae: 2.2248 - rmse: 2.7897 - val_loss: 2.2618 - val_mae: 2.2618 - val_rmse: 2.8206\n",
      "Epoch 113/150\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.2246 - mae: 2.2246 - rmse: 2.7864 - val_loss: 2.2586 - val_mae: 2.2586 - val_rmse: 2.8212\n",
      "Epoch 114/150\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.2236 - mae: 2.2236 - rmse: 2.7854 - val_loss: 2.2628 - val_mae: 2.2628 - val_rmse: 2.8210\n",
      "Epoch 115/150\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.2241 - mae: 2.2241 - rmse: 2.7895 - val_loss: 2.2679 - val_mae: 2.2679 - val_rmse: 2.8228\n",
      "Epoch 116/150\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.2237 - mae: 2.2237 - rmse: 2.7899 - val_loss: 2.2594 - val_mae: 2.2594 - val_rmse: 2.8248\n",
      "Epoch 117/150\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.2247 - mae: 2.2247 - rmse: 2.7851 - val_loss: 2.2556 - val_mae: 2.2556 - val_rmse: 2.8273\n",
      "Epoch 118/150\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.2223 - mae: 2.2223 - rmse: 2.7856 - val_loss: 2.2710 - val_mae: 2.2710 - val_rmse: 2.8257\n",
      "Epoch 119/150\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.2231 - mae: 2.2231 - rmse: 2.7916 - val_loss: 2.2700 - val_mae: 2.2700 - val_rmse: 2.8268\n",
      "Epoch 120/150\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.2221 - mae: 2.2221 - rmse: 2.7891 - val_loss: 2.2629 - val_mae: 2.2629 - val_rmse: 2.8280\n",
      "Epoch 121/150\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.2217 - mae: 2.2217 - rmse: 2.7866 - val_loss: 2.2593 - val_mae: 2.2593 - val_rmse: 2.8287\n",
      "Epoch 122/150\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.2207 - mae: 2.2207 - rmse: 2.7858 - val_loss: 2.2707 - val_mae: 2.2707 - val_rmse: 2.8265\n",
      "Epoch 123/150\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.2217 - mae: 2.2217 - rmse: 2.7907 - val_loss: 2.2711 - val_mae: 2.2711 - val_rmse: 2.8265\n",
      "Epoch 124/150\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.2209 - mae: 2.2209 - rmse: 2.7896 - val_loss: 2.2587 - val_mae: 2.2587 - val_rmse: 2.8291\n",
      "Epoch 125/150\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.2213 - mae: 2.2213 - rmse: 2.7852 - val_loss: 2.2640 - val_mae: 2.2640 - val_rmse: 2.8284\n",
      "Epoch 126/150\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.2206 - mae: 2.2206 - rmse: 2.7915 - val_loss: 2.2869 - val_mae: 2.2869 - val_rmse: 2.8287\n",
      "Epoch 127/150\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.2225 - mae: 2.2225 - rmse: 2.7975 - val_loss: 2.2692 - val_mae: 2.2692 - val_rmse: 2.8266\n",
      "Epoch 128/150\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.2214 - mae: 2.2214 - rmse: 2.7880 - val_loss: 2.2597 - val_mae: 2.2597 - val_rmse: 2.8278\n",
      "Epoch 129/150\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.2200 - mae: 2.2200 - rmse: 2.7883 - val_loss: 2.2845 - val_mae: 2.2845 - val_rmse: 2.8273\n",
      "Epoch 130/150\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.2216 - mae: 2.2216 - rmse: 2.7969 - val_loss: 2.2740 - val_mae: 2.2740 - val_rmse: 2.8260\n",
      "Epoch 131/150\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.2191 - mae: 2.2191 - rmse: 2.7890 - val_loss: 2.2608 - val_mae: 2.2608 - val_rmse: 2.8273\n",
      "Epoch 132/150\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.2191 - mae: 2.2191 - rmse: 2.7869 - val_loss: 2.2681 - val_mae: 2.2681 - val_rmse: 2.8270\n",
      "Epoch 133/150\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.2185 - mae: 2.2185 - rmse: 2.7880 - val_loss: 2.2666 - val_mae: 2.2666 - val_rmse: 2.8280\n",
      "Epoch 134/150\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.2186 - mae: 2.2186 - rmse: 2.7885 - val_loss: 2.2720 - val_mae: 2.2720 - val_rmse: 2.8267\n",
      "Epoch 135/150\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.2187 - mae: 2.2187 - rmse: 2.7890 - val_loss: 2.2611 - val_mae: 2.2611 - val_rmse: 2.8286\n",
      "Epoch 136/150\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.2187 - mae: 2.2187 - rmse: 2.7855 - val_loss: 2.2673 - val_mae: 2.2673 - val_rmse: 2.8279\n",
      "Epoch 137/150\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.2190 - mae: 2.2190 - rmse: 2.7904 - val_loss: 2.2885 - val_mae: 2.2885 - val_rmse: 2.8285\n",
      "Epoch 138/150\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.2211 - mae: 2.2211 - rmse: 2.7972 - val_loss: 2.2629 - val_mae: 2.2629 - val_rmse: 2.8283\n",
      "Epoch 139/150\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.2209 - mae: 2.2209 - rmse: 2.7852 - val_loss: 2.2584 - val_mae: 2.2584 - val_rmse: 2.8282\n",
      "Epoch 140/150\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.2192 - mae: 2.2192 - rmse: 2.7874 - val_loss: 2.2898 - val_mae: 2.2898 - val_rmse: 2.8292\n",
      "Epoch 141/150\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.2228 - mae: 2.2228 - rmse: 2.7988 - val_loss: 2.2729 - val_mae: 2.2729 - val_rmse: 2.8300\n",
      "Epoch 142/150\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.2206 - mae: 2.2206 - rmse: 2.7882 - val_loss: 2.2590 - val_mae: 2.2590 - val_rmse: 2.8333\n",
      "Epoch 143/150\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.2182 - mae: 2.2182 - rmse: 2.7860 - val_loss: 2.2815 - val_mae: 2.2815 - val_rmse: 2.8320\n",
      "Epoch 144/150\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.2196 - mae: 2.2196 - rmse: 2.7925 - val_loss: 2.2699 - val_mae: 2.2699 - val_rmse: 2.8323\n",
      "Epoch 145/150\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.2176 - mae: 2.2176 - rmse: 2.7867 - val_loss: 2.2722 - val_mae: 2.2722 - val_rmse: 2.8325\n",
      "Epoch 146/150\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 2.2182 - mae: 2.2182 - rmse: 2.7902 - val_loss: 2.2772 - val_mae: 2.2772 - val_rmse: 2.8305\n",
      "Epoch 147/150\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.2172 - mae: 2.2172 - rmse: 2.7892 - val_loss: 2.2632 - val_mae: 2.2632 - val_rmse: 2.8310\n",
      "Epoch 148/150\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.2179 - mae: 2.2179 - rmse: 2.7843 - val_loss: 2.2665 - val_mae: 2.2665 - val_rmse: 2.8294\n",
      "Epoch 149/150\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.2177 - mae: 2.2177 - rmse: 2.7879 - val_loss: 2.2856 - val_mae: 2.2856 - val_rmse: 2.8287\n",
      "Epoch 150/150\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.2197 - mae: 2.2197 - rmse: 2.7942 - val_loss: 2.2717 - val_mae: 2.2717 - val_rmse: 2.8277\n",
      "Test MAE=2.401  RMSE=2.798  R²=0.852\n"
     ]
    }
   ],
   "source": [
    "# Try default learning rate\n",
    "nn_model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss=\"mae\",\n",
    "    metrics=[tf.keras.metrics.RootMeanSquaredError(name=\"rmse\"), \"mae\"]\n",
    ")\n",
    "\n",
    "history = nn_model.fit(\n",
    "    Xtr, ytr,\n",
    "    validation_split=0.2,  \n",
    "    epochs=150,          \n",
    "    batch_size=32,\n",
    "    verbose=1     \n",
    ")\n",
    "\n",
    "# Evaluate test metrics\n",
    "test_loss, test_rmse, test_mae = nn_model.evaluate(Xte, yte, verbose=0)\n",
    "\n",
    "# Predict test set\n",
    "pred = nn_model.predict(Xte, verbose=0).ravel()\n",
    "\n",
    "# Compute R²\n",
    "r2 = r2_score(yte, pred)\n",
    "\n",
    "print(f\"Test MAE={test_mae:.3f}  RMSE={test_rmse:.3f}  R²={r2:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try redesigning the neural network.\n",
    "\n",
    "   - Input layer → 32 neurons → 16 neurons → Output (linear)\n",
    "   - Optimizer: Adam with learning rate = 1e-3\n",
    "   - Loss function: MAE (Mean Absolute Error)\n",
    "   - Metrics: RMSE and MAE\n",
    "   - Early stopping with patience=10, restoring best weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x319206660> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x319206660> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Neural Network (Scaled) | MAE=2.335  RMSE=2.828  R²=0.849\n"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential([\n",
    "    keras.layers.Input(shape=(Xtr.shape[1],)),   \n",
    "    keras.layers.Dense(32, activation=\"relu\"),\n",
    "    keras.layers.Dense(16, activation=\"relu\"),\n",
    "    keras.layers.Dense(1, activation=\"linear\")\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n",
    "    loss=\"mae\",\n",
    "    metrics=[tf.keras.metrics.RootMeanSquaredError(name=\"rmse\"), \"mae\"]\n",
    ")\n",
    "\n",
    "\n",
    "early_stop = keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    patience=10,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "hist = model.fit(\n",
    "    Xtr, ytr,\n",
    "    validation_split=0.2,\n",
    "    epochs=150,\n",
    "    batch_size=16,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "# Predict exam scores on unseen test data\n",
    "pred = model.predict(Xte, verbose=0).ravel()\n",
    "\n",
    "# Calculate regression metrics\n",
    "mae  = mean_absolute_error(yte, pred)\n",
    "rmse = root_mean_squared_error(yte, pred)\n",
    "r2   = r2_score(yte, pred)\n",
    "\n",
    "print(f\"Neural Network (Scaled) | MAE={mae:.3f}  RMSE={rmse:.3f}  R²={r2:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's experiment with different learning rates and batch sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Learning rates tested: [1e-2, 1e-3, 1e-4]\n",
    "- Batch sizes tested: [8, 16, 32, 64]\n",
    "- Epochs: 150 with early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR=0.01   | Batch=8   | MAE=2.601  RMSE=3.169  R²=0.811\n",
      "LR=0.01   | Batch=16  | MAE=12.077  RMSE=14.126  R²=-2.760\n",
      "LR=0.01   | Batch=32  | MAE=6.358  RMSE=7.656  R²=-0.104\n",
      "LR=0.01   | Batch=64  | MAE=24.765  RMSE=25.804  R²=-11.546\n",
      "LR=0.001  | Batch=8   | MAE=29.717  RMSE=30.634  R²=-16.683\n",
      "LR=0.001  | Batch=16  | MAE=39.529  RMSE=40.336  R²=-29.657\n",
      "LR=0.001  | Batch=32  | MAE=44.469  RMSE=45.260  R²=-37.600\n",
      "LR=0.001  | Batch=64  | MAE=46.952  RMSE=47.743  R²=-41.950\n",
      "LR=0.0001 | Batch=8   | MAE=47.465  RMSE=48.257  R²=-42.881\n",
      "LR=0.0001 | Batch=16  | MAE=48.458  RMSE=49.252  R²=-44.708\n",
      "LR=0.0001 | Batch=32  | MAE=48.959  RMSE=49.753  R²=-45.643\n",
      "LR=0.0001 | Batch=64  | MAE=49.213  RMSE=50.008  R²=-46.122\n",
      "\n",
      "Best combination:\n",
      "LR=0.01 | Batch=8 | MAE=2.601  RMSE=3.169  R²=0.811\n"
     ]
    }
   ],
   "source": [
    "def build_model(input_dim, lr):\n",
    "    model = keras.Sequential([\n",
    "        keras.layers.Input(shape=(input_dim,)),\n",
    "        keras.layers.Dense(32, activation=\"relu\"),\n",
    "        keras.layers.Dense(16, activation=\"relu\"),\n",
    "        keras.layers.Dense(1, activation=\"linear\")\n",
    "    ])\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=lr),\n",
    "        loss=\"mae\",\n",
    "        metrics=[tf.keras.metrics.RootMeanSquaredError(name=\"rmse\"), \"mae\"]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "early_stop = keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    patience=10,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "learning_rates = [1e-2, 1e-3, 1e-4]\n",
    "batch_sizes = [8, 16, 32, 64]\n",
    "\n",
    "results = []\n",
    "\n",
    "for lr in learning_rates:\n",
    "    for bs in batch_sizes:\n",
    "        tf.keras.utils.set_random_seed(42)\n",
    "\n",
    "        model = build_model(Xtr.shape[1], lr)\n",
    "        hist = model.fit(\n",
    "            Xtr, ytr,\n",
    "            validation_split=0.2,\n",
    "            epochs=150,\n",
    "            batch_size=bs,\n",
    "            verbose=0,\n",
    "            callbacks=[early_stop]\n",
    "        )\n",
    "\n",
    "        pred = model.predict(Xte, verbose=0).ravel()\n",
    "        mae  = mean_absolute_error(yte, pred)\n",
    "        rmse = root_mean_squared_error(yte, pred)\n",
    "        r2   = r2_score(yte, pred)\n",
    "\n",
    "        results.append((lr, bs, mae, rmse, r2))\n",
    "        print(f\"LR={lr:<6} | Batch={bs:<3} | MAE={mae:.3f}  RMSE={rmse:.3f}  R²={r2:.3f}\")\n",
    "\n",
    "# --- summarize best combo ---\n",
    "best = min(results, key=lambda x: x[2])  # smallest MAE\n",
    "print(\"\\nBest combination:\")\n",
    "print(f\"LR={best[0]} | Batch={best[1]} | MAE={best[2]:.3f}  RMSE={best[3]:.3f}  R²={best[4]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizing Feature Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let's apply z-score normalization: `(X - μ) / σ`\n",
    "- Using training set statistics only to prevent data leakage\n",
    "- Same hyperparameter grid search performed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR=0.01   | Batch=8   | MAE=3.456  RMSE=3.980  R²=0.701\n",
      "LR=0.01   | Batch=16  | MAE=32.845  RMSE=33.669  R²=-20.361\n",
      "LR=0.01   | Batch=32  | MAE=33.806  RMSE=34.599  R²=-21.557\n",
      "LR=0.01   | Batch=64  | MAE=34.288  RMSE=35.070  R²=-22.175\n",
      "LR=0.001  | Batch=8   | MAE=34.450  RMSE=35.226  R²=-22.382\n",
      "LR=0.001  | Batch=16  | MAE=34.620  RMSE=35.390  R²=-22.600\n",
      "LR=0.001  | Batch=32  | MAE=34.719  RMSE=35.485  R²=-22.727\n",
      "LR=0.001  | Batch=64  | MAE=34.773  RMSE=35.538  R²=-22.798\n",
      "LR=0.0001 | Batch=8   | MAE=34.791  RMSE=35.555  R²=-22.820\n",
      "LR=0.0001 | Batch=16  | MAE=34.809  RMSE=35.572  R²=-22.844\n",
      "LR=0.0001 | Batch=32  | MAE=34.820  RMSE=35.582  R²=-22.857\n",
      "LR=0.0001 | Batch=64  | MAE=34.825  RMSE=35.588  R²=-22.864\n",
      "\n",
      "Best combination:\n",
      "LR=0.01 | Batch=8 | MAE=3.456  RMSE=3.980  R²=0.701\n"
     ]
    }
   ],
   "source": [
    "# --- Standardize using training stats ---\n",
    "mu = X_train.mean()\n",
    "sd = X_train.std().replace(0, 1)\n",
    "\n",
    "Xtr = ((X_train - mu) / sd).to_numpy(dtype=np.float32)\n",
    "Xte = ((X_test  - mu) / sd).to_numpy(dtype=np.float32)\n",
    "ytr = y_train.to_numpy(dtype=np.float32)\n",
    "yte = y_test.to_numpy(dtype=np.float32)\n",
    "\n",
    "# --- Model builder ---\n",
    "def build_model(input_dim, lr):\n",
    "    model = keras.Sequential([\n",
    "        keras.layers.Input(shape=(input_dim,)),\n",
    "        keras.layers.Dense(32, activation=\"relu\"),\n",
    "        keras.layers.Dense(16, activation=\"relu\"),\n",
    "        keras.layers.Dense(1, activation=\"linear\")\n",
    "    ])\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=lr),\n",
    "        loss=\"mae\",\n",
    "        metrics=[tf.keras.metrics.RootMeanSquaredError(name=\"rmse\"), \"mae\"]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "early_stop = keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    patience=10,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "# --- Grid search parameters ---\n",
    "learning_rates = [1e-2, 1e-3, 1e-4]\n",
    "batch_sizes = [8, 16, 32, 64]\n",
    "results = []\n",
    "\n",
    "for lr in learning_rates:\n",
    "    for bs in batch_sizes:\n",
    "        tf.keras.utils.set_random_seed(42)\n",
    "\n",
    "        model = build_model(Xtr.shape[1], lr)\n",
    "        model.fit(\n",
    "            Xtr, ytr,\n",
    "            validation_split=0.2,\n",
    "            epochs=150,\n",
    "            batch_size=bs,\n",
    "            verbose=0,\n",
    "            callbacks=[early_stop]\n",
    "        )\n",
    "\n",
    "        pred = model.predict(Xte, verbose=0).ravel()\n",
    "        mae  = mean_absolute_error(yte, pred)\n",
    "        rmse = root_mean_squared_error(yte, pred)\n",
    "        r2   = r2_score(yte, pred)\n",
    "\n",
    "        results.append((lr, bs, mae, rmse, r2))\n",
    "        print(f\"LR={lr:<6} | Batch={bs:<3} | MAE={mae:.3f}  RMSE={rmse:.3f}  R²={r2:.3f}\")\n",
    "\n",
    "best = min(results, key=lambda x: x[2])  # lowest MAE\n",
    "print(\"\\nBest combination:\")\n",
    "print(f\"LR={best[0]} | Batch={best[1]} | MAE={best[2]:.3f}  RMSE={best[3]:.3f}  R²={best[4]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/metrics/_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1: MAE=3.129, RMSE=3.709, R²=0.741\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/metrics/_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2: MAE=3.421, RMSE=4.203, R²=0.605\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/metrics/_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3: MAE=3.204, RMSE=3.857, R²=0.662\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/metrics/_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4: MAE=3.301, RMSE=3.888, R²=0.600\n",
      "Fold 5: MAE=2.792, RMSE=3.390, R²=0.764\n",
      "\n",
      "Average 5-Fold Results:\n",
      "MAE=3.169 ± 0.213\n",
      "RMSE=3.809 ± 0.265\n",
      "R²=0.674 ± 0.068\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/metrics/_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "X_np = X.values.astype(np.float32)\n",
    "y_np = y.values.astype(np.float32)\n",
    "\n",
    "# Build model\n",
    "def build_model(input_dim):\n",
    "    m = keras.Sequential([\n",
    "        keras.layers.Input(shape=(input_dim,)),\n",
    "        keras.layers.Dense(32, activation=\"relu\"),\n",
    "        keras.layers.Dense(16, activation=\"relu\"),\n",
    "        keras.layers.Dense(1, activation=\"linear\")\n",
    "    ])\n",
    "    m.compile(optimizer=keras.optimizers.Adam(1e-3), loss=\"mae\")\n",
    "    return m\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "maes, rmses, r2s = [], [], []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X_np), 1):\n",
    "    X_tr, X_val = X_np[train_idx], X_np[val_idx]\n",
    "    y_tr, y_val = y_np[train_idx], y_np[val_idx]\n",
    "\n",
    "    # Standardize (train stats only)\n",
    "    mu = X_tr.mean(axis=0, keepdims=True)\n",
    "    sd = X_tr.std(axis=0, keepdims=True)\n",
    "    sd[sd == 0] = 1\n",
    "    X_tr = (X_tr - mu) / sd\n",
    "    X_val = (X_val - mu) / sd\n",
    "\n",
    "    # Train\n",
    "    tf.keras.utils.set_random_seed(42)\n",
    "    model = build_model(X_tr.shape[1])\n",
    "    model.fit(X_tr, y_tr, epochs=120, batch_size=16, verbose=0)\n",
    "\n",
    "    # Predict & metrics\n",
    "    pred = model.predict(X_val, verbose=0).ravel()\n",
    "    mae  = mean_absolute_error(y_val, pred)\n",
    "    rmse = mean_squared_error(y_val, pred, squared=False)\n",
    "    r2   = r2_score(y_val, pred)\n",
    "\n",
    "    maes.append(mae); rmses.append(rmse); r2s.append(r2)\n",
    "    print(f\"Fold {fold}: MAE={mae:.3f}, RMSE={rmse:.3f}, R²={r2:.3f}\")\n",
    "\n",
    "# --- Overall averages ---\n",
    "print(\"\\nAverage 5-Fold Results:\")\n",
    "print(f\"MAE={np.mean(maes):.3f} ± {np.std(maes):.3f}\")\n",
    "print(f\"RMSE={np.mean(rmses):.3f} ± {np.std(rmses):.3f}\")\n",
    "print(f\"R²={np.mean(r2s):.3f} ± {np.std(r2s):.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
